---
phase: 05-validation-delivery
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/dataset_generator/integration/__init__.py
  - src/dataset_generator/integration/langfuse_client.py
  - src/dataset_generator/cli.py
  - pyproject.toml
autonomous: true
user_setup:
  - service: langfuse
    why: "Dataset upload and experiment tracking"
    env_vars:
      - name: LANGFUSE_PUBLIC_KEY
        source: "Langfuse Dashboard -> Settings -> API Keys"
      - name: LANGFUSE_SECRET_KEY
        source: "Langfuse Dashboard -> Settings -> API Keys"
      - name: LANGFUSE_HOST
        source: "Optional: custom Langfuse instance URL (defaults to https://cloud.langfuse.com)"

must_haves:
  truths:
    - "User can run `python -m dataset_generator upload --out <dir> --dataset-name <name>` to upload to Langfuse"
    - "Upload creates a Langfuse dataset with all examples as dataset items"
    - "Each dataset item includes input messages, expected_output, and metadata (use_case_id, test_case_id, policy_ids, evaluation_criteria)"
    - "Upload fails gracefully with clear message if LANGFUSE env vars not set"
    - "Langfuse is an optional dependency — validation and generation work without it"
  artifacts:
    - path: "src/dataset_generator/integration/langfuse_client.py"
      provides: "Langfuse dataset upload with experiment tracking metadata"
      contains: "def upload_to_langfuse"
    - path: "src/dataset_generator/cli.py"
      provides: "upload command for Langfuse integration"
      contains: "def upload"
    - path: "pyproject.toml"
      provides: "langfuse as optional dependency"
      contains: "langfuse"
  key_links:
    - from: "src/dataset_generator/cli.py"
      to: "src/dataset_generator/integration/langfuse_client.py"
      via: "upload command calls upload_to_langfuse"
      pattern: "upload_to_langfuse"
    - from: "src/dataset_generator/integration/langfuse_client.py"
      to: "langfuse SDK"
      via: "Langfuse client create_dataset and create_dataset_item"
      pattern: "langfuse\\.create_dataset"
---

<objective>
Implement Langfuse integration for uploading generated datasets as Langfuse dataset items with experiment tracking metadata.

Purpose: Enable users to use generated datasets directly in Langfuse for LLM evaluation experiments (INTG-01)
Output: Working `python -m dataset_generator upload --out <dir> --dataset-name <name>` command
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-validation-delivery/05-RESEARCH.md

@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/cli.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Langfuse integration module and upload CLI command</name>
  <files>
    src/dataset_generator/integration/__init__.py
    src/dataset_generator/integration/langfuse_client.py
    src/dataset_generator/cli.py
    pyproject.toml
  </files>
  <action>
**pyproject.toml** — Add langfuse as optional dependency:
- Add `langfuse` to `[project.optional-dependencies]` section under a new `langfuse` key: `langfuse = ["langfuse>=2.0"]`
- Keep it OUT of the required `dependencies` list so validation/generation still works without it
- This allows: `pip install dataset-generator[langfuse]`

**src/dataset_generator/integration/__init__.py** — empty or export upload_to_langfuse

**src/dataset_generator/integration/langfuse_client.py** — Core upload function:

```python
def upload_to_langfuse(
    dataset_name: str,
    examples: list[DatasetExample],
    public_key: str,
    secret_key: str,
    host: str | None = None,
) -> dict:
```

Implementation:
1. Import langfuse lazily: `from langfuse import Langfuse` inside the function body. If import fails, raise ImportError with message "langfuse not installed. Run: pip install dataset-generator[langfuse]"
2. Create Langfuse client with provided credentials and optional host
3. Call `langfuse.create_dataset(name=dataset_name, description=..., metadata={generator, version, timestamp})`
4. For each example, call `langfuse.create_dataset_item()` with:
   - `dataset_name=dataset_name`
   - `input={"messages": [m.model_dump() for m in example.input.messages], "case": example.case, "format": example.format}`
   - `expected_output=example.expected_output`
   - `metadata={"use_case_id": example.use_case_id, "test_case_id": example.test_case_id, "policy_ids": example.policy_ids, "evaluation_criteria": example.evaluation_criteria, **example.metadata}`
5. Call `langfuse.flush()` to ensure all items are uploaded (critical — SDK batches by default)
6. Return dict with `{"dataset_name": name, "items_uploaded": count, "status": "success"}`

**src/dataset_generator/cli.py** — Add `upload` command:

```python
@app.command("upload")
def upload(
    out: Path = typer.Option(..., "--out", exists=True, dir_okay=True, file_okay=False, help="Output directory with dataset.json"),
    dataset_name: str = typer.Option(..., "--dataset-name", help="Name for the Langfuse dataset"),
    host: str = typer.Option(None, "--langfuse-host", help="Custom Langfuse host URL"),
) -> None:
```

Implementation:
1. Read LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY from environment (already loaded by dotenv at top of cli.py)
2. If either key is missing, print clear error message: "Langfuse credentials not configured. Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in your .env file." and exit with code 1
3. Load dataset.json from out dir using `DatasetExampleList.model_validate_json()`
4. Call `upload_to_langfuse()` with loaded examples and credentials
5. Print success summary: dataset name, items uploaded
6. If host not provided, check LANGFUSE_HOST env var as fallback, then default to None (SDK uses cloud.langfuse.com)
7. Handle ImportError for missing langfuse package with helpful install message
8. Handle any langfuse API errors gracefully with typer.echo to stderr

NOTE: The upload command is intentionally separate from validate — they serve different purposes and have different dependency requirements.
  </action>
  <verify>
`python -c "from dataset_generator.integration.langfuse_client import upload_to_langfuse; print('import OK')"` succeeds.
`python -m dataset_generator upload --help` shows --out, --dataset-name, --langfuse-host parameters.
`python -m dataset_generator upload --out out/support --dataset-name test 2>&1; echo "exit: $?"` — should fail gracefully with "Langfuse credentials not configured" message if env vars not set.
  </verify>
  <done>
Upload command exists, loads dataset.json, uploads to Langfuse with proper metadata structure (input messages, expected_output, evaluation_criteria in metadata), flush ensures delivery, graceful failure when credentials missing, langfuse is optional dependency.
  </done>
</task>

</tasks>

<verification>
1. `python -m dataset_generator upload --help` — shows all parameters with descriptions
2. Without LANGFUSE env vars: `python -m dataset_generator upload --out out/support --dataset-name test` prints credential error, exits 1
3. Module imports work: `python -c "from dataset_generator.integration import langfuse_client"`
4. Optional dependency in pyproject.toml: `pip install -e ".[langfuse]"` installs langfuse
</verification>

<success_criteria>
- upload command accepts --out and --dataset-name parameters
- Langfuse client uploads all examples with proper input/output/metadata structure
- langfuse.flush() is called after upload to ensure delivery
- Missing credentials produce clear error message (not stack trace)
- langfuse is optional — core functionality works without it installed
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-delivery/05-02-SUMMARY.md`
</output>
