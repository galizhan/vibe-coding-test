---
phase: 05-validation-delivery
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/dataset_generator/validation/__init__.py
  - src/dataset_generator/validation/validator.py
  - src/dataset_generator/validation/integrity_checker.py
  - src/dataset_generator/validation/report.py
  - src/dataset_generator/cli.py
autonomous: true

must_haves:
  truths:
    - "User can run `python -m dataset_generator validate --out out/support` and see a summary report"
    - "Validation exits with code 0 when all checks pass"
    - "Validation exits with code 1 when referential integrity errors exist"
    - "Validation checks use_case_id, policy_ids, test_case_id links across all JSON files"
    - "Evidence quote mismatches produce warnings but do not cause validation failure"
  artifacts:
    - path: "src/dataset_generator/validation/validator.py"
      provides: "Main validation orchestrator loading all JSON and running checks"
      contains: "class DatasetValidator"
    - path: "src/dataset_generator/validation/integrity_checker.py"
      provides: "Cross-model referential integrity checking"
      contains: "def check_referential_integrity"
    - path: "src/dataset_generator/validation/report.py"
      provides: "Structured validation report with counts, errors, warnings"
      contains: "class ValidationResult"
    - path: "src/dataset_generator/cli.py"
      provides: "validate command with --out parameter and exit codes"
      contains: "def validate"
  key_links:
    - from: "src/dataset_generator/cli.py"
      to: "src/dataset_generator/validation/validator.py"
      via: "validate command calls DatasetValidator"
      pattern: "DatasetValidator.*validate"
    - from: "src/dataset_generator/validation/validator.py"
      to: "src/dataset_generator/models/__init__.py"
      via: "loads JSON files as Pydantic models"
      pattern: "model_validate_json"
    - from: "src/dataset_generator/validation/validator.py"
      to: "src/dataset_generator/validation/integrity_checker.py"
      via: "delegates referential integrity checks"
      pattern: "check_referential_integrity"
---

<objective>
Implement the `validate` CLI command that loads generated JSON artifacts, validates them against Pydantic data contracts, checks referential integrity across models, validates evidence quotes, and prints a structured summary report with proper exit codes.

Purpose: Enable users to verify generated dataset quality without re-running the pipeline (VALD-01, VALD-02, VALD-03)
Output: Working `python -m dataset_generator validate --out <dir>` command
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-validation-delivery/05-RESEARCH.md

@src/dataset_generator/models/__init__.py
@src/dataset_generator/models/use_case.py
@src/dataset_generator/models/policy.py
@src/dataset_generator/models/test_case.py
@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/cli.py
@src/dataset_generator/generation/coverage.py
@src/dataset_generator/extraction/evidence_validator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation module with orchestrator, integrity checker, and report</name>
  <files>
    src/dataset_generator/validation/__init__.py
    src/dataset_generator/validation/validator.py
    src/dataset_generator/validation/integrity_checker.py
    src/dataset_generator/validation/report.py
  </files>
  <action>
Create `src/dataset_generator/validation/` package with 3 modules:

**report.py** — `ValidationResult` class:
- `errors: list[str]` — structural/integrity errors (cause exit code 1)
- `warnings: list[str]` — evidence mismatches, orphaned items (informational)
- `counts: dict[str, int]` — artifact counts (use_cases, policies, test_cases, examples)
- `formats: list[str]` — detected format types from examples
- `is_valid` property: True if `len(errors) == 0`
- `add_error(msg)`, `add_warning(msg)`, `set_count(key, val)` methods
- `print_report()` method: prints formatted summary to stdout using typer.echo, prints errors to stderr

**integrity_checker.py** — `check_referential_integrity()` function:
- Takes loaded UseCaseList, PolicyList, TestCaseList, DatasetExampleList
- Builds lookup sets: `use_case_ids`, `policy_ids`, `test_case_ids`
- Checks forward references:
  - test_case.use_case_id -> use_case_ids
  - example.use_case_id -> use_case_ids
  - example.test_case_id -> test_case_ids
  - example.policy_ids items -> policy_ids (this is the key addition vs existing coverage.py — checks against ACTUAL loaded policy IDs, not just prefix format)
- Returns `list[str]` of error messages (empty if valid)
- NOTE: The existing `coverage.py:check_referential_integrity()` only checks pol_ prefix format; this new function validates against the actual set of policy IDs loaded from policies.json

**validator.py** — `DatasetValidator` class:
- `__init__(self, out_dir: Path)` stores path and creates `ValidationResult`
- `validate() -> ValidationResult` runs all checks in order:
  1. Check required files exist: use_cases.json, policies.json, test_cases.json, dataset.json
  2. Load each file via `model_class.model_validate_json(path.read_text())`, catching `ValidationError` per-file and reporting field-level errors with file path context
  3. If all 4 models loaded successfully, call `check_referential_integrity()` and add results as errors
  4. Evidence validation (optional, only if input source available): reuse existing `validate_evidence_quote` from extraction module, add results as WARNINGS not errors (per project decision: evidence warns but doesn't fail)
  5. Record counts in result
- Store loaded models as instance attributes for potential reuse by Langfuse upload
- Use Pydantic's `model_validate_json()` (not json.load + model_validate) for efficiency

**__init__.py** — export DatasetValidator, ValidationResult, check_referential_integrity
  </action>
  <verify>
`python -c "from dataset_generator.validation import DatasetValidator, ValidationResult; print('imports OK')"` succeeds.
`python -c "from dataset_generator.validation.integrity_checker import check_referential_integrity; print('integrity OK')"` succeeds.
  </verify>
  <done>
Validation module exists with DatasetValidator orchestrator, integrity checker that validates all cross-model ID references (including policy_ids against actual loaded policies), and ValidationResult with structured report output.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire validate command into CLI with exit codes</name>
  <files>
    src/dataset_generator/cli.py
  </files>
  <action>
Replace the existing stub `validate` command in `cli.py` with a full implementation:

```python
@app.command("validate")
def validate(
    out: Path = typer.Option(
        ..., "--out", exists=True, dir_okay=True, file_okay=False,
        help="Output directory to validate"
    ),
) -> None:
```

Implementation:
1. Import `DatasetValidator` from `dataset_generator.validation`
2. Create `DatasetValidator(out)` and call `result = validator.validate()`
3. Call `result.print_report()` to display summary
4. If `result.is_valid`: print success message and `raise typer.Exit(code=0)`
5. If not valid: print error count to stderr and `raise typer.Exit(code=1)`

Use `typer.Exit` (not `sys.exit`) for proper Typer integration and testability. The `--out` parameter should use `exists=True` so Typer validates the directory exists before our code runs. Do NOT require `--input` for the validate command — it only needs the output directory.

Test by running against `out/support/`:
```bash
python -m dataset_generator validate --out out/support
```
Expected: prints summary with counts of use_cases, policies, test_cases, examples, any warnings, exits 0 if no errors.
  </action>
  <verify>
`python -m dataset_generator validate --out out/support` prints summary report and exits with code 0 (assuming valid data).
`python -m dataset_generator validate --out /nonexistent 2>&1; echo "exit: $?"` shows Typer error about missing directory.
`python -m dataset_generator validate --help` shows --out parameter documentation.
  </verify>
  <done>
validate command accepts --out, loads all JSON files, checks schema compliance and referential integrity, prints summary report with counts/errors/warnings, exits 0 on success and 1 on errors.
  </done>
</task>

</tasks>

<verification>
1. `python -m dataset_generator validate --out out/support` — prints summary, exits 0
2. `python -m dataset_generator validate --help` — shows --out parameter
3. Create a temporary directory with invalid JSON and verify exit code > 0
4. Referential integrity: if an example references a non-existent policy_id, it should appear as an error
</verification>

<success_criteria>
- validate command is a working Typer command with --out parameter
- Validation loads all 4 JSON files via Pydantic model_validate_json
- Referential integrity checks policy_ids against actual loaded policies (not just prefix format)
- Evidence validation results appear as warnings, not errors
- Exit code 0 on success, 1 on validation errors
- Summary report shows counts, errors, and warnings
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-delivery/05-01-SUMMARY.md`
</output>
