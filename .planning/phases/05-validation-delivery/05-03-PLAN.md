---
phase: 05-validation-delivery
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - out/support/use_cases.json
  - out/support/policies.json
  - out/support/test_cases.json
  - out/support/dataset.json
  - out/support/run_manifest.json
  - out/operator_quality/use_cases.json
  - out/operator_quality/policies.json
  - out/operator_quality/test_cases.json
  - out/operator_quality/dataset.json
  - out/operator_quality/run_manifest.json
  - README.md
autonomous: true

must_haves:
  truths:
    - "out/support/ contains use_cases.json, policies.json, test_cases.json, dataset.json, run_manifest.json"
    - "out/operator_quality/ contains use_cases.json, policies.json, test_cases.json, dataset.json, run_manifest.json"
    - "Both output directories pass `python -m dataset_generator validate --out <dir>` with exit code 0"
    - "README.md has setup instructions, dependencies list, environment variable table, and usage examples for generate, validate, and upload commands"
  artifacts:
    - path: "out/support/dataset.json"
      provides: "Pre-generated support bot dataset examples"
    - path: "out/operator_quality/dataset.json"
      provides: "Pre-generated operator quality dataset examples"
    - path: "README.md"
      provides: "Complete project documentation with setup and usage instructions"
      contains: "OPENAI_API_KEY"
  key_links:
    - from: "README.md"
      to: "src/dataset_generator/cli.py"
      via: "documents all CLI commands"
      pattern: "python -m dataset_generator"
    - from: "out/support/"
      to: "validate command"
      via: "pre-generated artifacts pass validation"
      pattern: "validate --out out/support"
---

<objective>
Generate pre-built output artifacts for support bot and operator quality cases, validate them, and create comprehensive README documentation.

Purpose: Deliver ready-to-use artifacts and documentation so users can immediately explore the project (DLVR-01, DLVR-02)
Output: Populated out/support/ and out/operator_quality/ directories plus README.md
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-validation-delivery/05-RESEARCH.md

@src/dataset_generator/cli.py
@pyproject.toml

# Need validate command from Plan 01
@.planning/phases/05-validation-delivery/05-01-SUMMARY.md
# Need upload command docs from Plan 02
@.planning/phases/05-validation-delivery/05-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate and validate pre-built output artifacts</name>
  <files>
    out/support/use_cases.json
    out/support/policies.json
    out/support/test_cases.json
    out/support/dataset.json
    out/support/run_manifest.json
    out/operator_quality/use_cases.json
    out/operator_quality/policies.json
    out/operator_quality/test_cases.json
    out/operator_quality/dataset.json
    out/operator_quality/run_manifest.json
  </files>
  <action>
Generate pre-built artifacts for both use cases by running the pipeline:

**Step 1: Generate support bot artifacts** (out/support/ already has data â€” regenerate to ensure current pipeline compatibility):
```bash
python -m dataset_generator generate \
  --input example_input_raw_support_faq_and_tickets.md \
  --out out/support \
  --seed 42 \
  --model gpt-4o-mini
```

**Step 2: Generate operator quality artifacts** (out/operator_quality/ is currently empty):
```bash
python -m dataset_generator generate \
  --input example_input_raw_operator_quality_checks.md \
  --out out/operator_quality \
  --seed 42 \
  --model gpt-4o-mini
```

**Step 3: Validate both outputs** (using the validate command from Plan 01):
```bash
python -m dataset_generator validate --out out/support
python -m dataset_generator validate --out out/operator_quality
```

Both must exit with code 0. If validation fails:
- Check error messages for referential integrity issues
- If evidence warnings appear, that's OK (warnings don't fail validation)
- If schema errors, debug and re-run generation

**Important notes:**
- Use --seed 42 for both runs to ensure reproducible outputs
- The pipeline requires OPENAI_API_KEY to be set in environment
- If generation takes too long, use `--n-use-cases 5 --n-test-cases-per-uc 3 --n-examples-per-tc 1` (defaults)
- Keep quality_report.html in out/support/ if it was generated (don't delete it)
  </action>
  <verify>
`python -m dataset_generator validate --out out/support; echo "exit: $?"` prints summary and exits 0.
`python -m dataset_generator validate --out out/operator_quality; echo "exit: $?"` prints summary and exits 0.
Both directories contain at minimum: use_cases.json, policies.json, test_cases.json, dataset.json, run_manifest.json.
  </verify>
  <done>
Pre-generated artifacts exist in out/support/ and out/operator_quality/ and both pass validation with exit code 0.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive README with setup, usage, and environment documentation</name>
  <files>
    README.md
  </files>
  <action>
Create `README.md` at project root with these sections:

**1. Title and Description**
- "Synthetic Dataset Generator for LLM Agent Testing"
- Brief description: transforms raw markdown business requirements into validated synthetic test datasets with full traceability

**2. Features**
- Bulleted list of key capabilities:
  - Extract use cases and policies from unstructured markdown
  - Generate test cases with parameter variations
  - Produce evaluation-ready dataset examples
  - Multi-format support (single_turn_qa, single_utterance_correction, dialog_last_turn_correction)
  - Auto-detect case type (support_bot, operator_quality, doctor_booking)
  - Built-in validation with referential integrity checks
  - Optional Langfuse integration for experiment tracking
  - Evidence traceability back to source document lines

**3. Quick Start**
- Clone, install, set env var, run generate command
- Example with support bot input

**4. Installation**
```bash
pip install -e .
# With optional Langfuse support:
pip install -e ".[langfuse]"
```

**5. Environment Variables**
Table format with Required vs Optional sections:

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `OPENAI_API_KEY` | Yes | OpenAI API key for LLM generation | `sk-proj-...` |
| `LANGFUSE_PUBLIC_KEY` | No | Langfuse public API key | `pk-lf-...` |
| `LANGFUSE_SECRET_KEY` | No | Langfuse secret API key | `sk-lf-...` |
| `LANGFUSE_HOST` | No | Custom Langfuse instance URL | `https://cloud.langfuse.com` |

Include `.env` file example:
```bash
# Required
OPENAI_API_KEY=sk-proj-...

# Optional: Langfuse Integration
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
```

**6. Usage**
Three subsections with full command examples:

**Generate:**
```bash
python -m dataset_generator generate --input <file.md> --out <dir> [--seed 42] [--model gpt-4o-mini]
```
Document all flags: --input, --out, --seed, --model, --n-use-cases, --n-test-cases-per-uc, --n-examples-per-tc

**Validate:**
```bash
python -m dataset_generator validate --out <dir>
```
Explain exit codes: 0 = success, 1 = validation errors. Mention that evidence mismatches are warnings only.

**Upload to Langfuse:**
```bash
python -m dataset_generator upload --out <dir> --dataset-name <name> [--langfuse-host <url>]
```
Note: requires LANGFUSE env vars to be set.

**7. Pre-generated Artifacts**
- Explain out/support/ and out/operator_quality/ directories
- List files in each (use_cases.json, policies.json, test_cases.json, dataset.json, run_manifest.json)
- Mention they can be validated with the validate command

**8. Supported Use Cases**
Brief description of each:
- Support Bot (support_bot): FAQ and ticket-based customer support
- Operator Quality (operator_quality): Message correction for operators
- Doctor Booking (doctor_booking): Medical appointment booking

**9. Architecture** (brief)
- Pipeline: markdown -> use cases -> policies -> test cases -> dataset examples
- Full traceability via evidence quotes and ID references (uc_ -> tc_ -> ex_)

**10. Dependencies**
- Python >= 3.10
- Key libraries: pydantic, openai, typer, python-dotenv, tenacity, rapidfuzz
- Optional: langfuse (for dataset upload)

**Style notes:**
- No emojis
- Use fenced code blocks with bash/python syntax highlighting
- Keep it practical and focused on getting started
- Do NOT include development/contributing sections (this is a deliverable, not an open-source project)
  </action>
  <verify>
README.md exists at project root and contains all required sections.
`grep "OPENAI_API_KEY" README.md` finds environment variable documentation.
`grep "validate" README.md` finds validate command documentation.
`grep "upload" README.md` finds upload command documentation.
`grep "out/support" README.md` finds pre-generated artifacts section.
  </verify>
  <done>
README.md provides complete setup instructions, all environment variables documented, all 3 CLI commands (generate, validate, upload) with examples, pre-generated artifact locations, and architecture overview.
  </done>
</task>

</tasks>

<verification>
1. `python -m dataset_generator validate --out out/support` exits 0
2. `python -m dataset_generator validate --out out/operator_quality` exits 0
3. Both out/ directories contain 5 required JSON files
4. README.md exists with OPENAI_API_KEY, validate, upload, and out/support documentation
5. A new user could follow README instructions to set up and run the project
</verification>

<success_criteria>
- out/support/ has complete, validated artifacts
- out/operator_quality/ has complete, validated artifacts
- README.md has setup instructions, environment variables, and all CLI command documentation
- Both output directories pass validation with exit code 0
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-delivery/05-03-SUMMARY.md`
</output>
