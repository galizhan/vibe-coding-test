---
phase: 03-test-dataset-generation
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - src/dataset_generator/generation/orchestrator.py
  - src/dataset_generator/generation/coverage.py
  - src/dataset_generator/pipeline.py
  - src/dataset_generator/cli.py
  - src/dataset_generator/utils/file_writer.py
autonomous: true

must_haves:
  truths:
    - "OpenAI function calling selects which framework to invoke based on task context (not a fixed pipeline)"
    - "Coverage enforcement ensures minimum 3 test cases per use case and 3+ evaluation criteria per example"
    - "Pipeline orchestrates end-to-end: parse markdown -> extract use cases/policies -> generate test cases -> generate dataset examples -> write all outputs"
    - "run_manifest.json is written per run with input_path, out_path, seed, timestamp, generator_version, llm block"
    - "If a framework call fails, fallback to direct OpenAI generation ensures pipeline always completes"
  artifacts:
    - path: "src/dataset_generator/generation/orchestrator.py"
      provides: "OpenAI function calling router with tool definitions for DeepEval, Ragas, Giskard, and fallback logic"
      contains: "tool_choice"
    - path: "src/dataset_generator/generation/coverage.py"
      provides: "Coverage enforcement ensuring minimum test cases per UC and criteria per example"
      contains: "enforce_coverage"
    - path: "src/dataset_generator/pipeline.py"
      provides: "Updated pipeline with test case and dataset example generation steps and run_manifest.json output"
      contains: "run_manifest"
    - path: "src/dataset_generator/cli.py"
      provides: "Updated CLI that invokes full generation pipeline"
      contains: "test_cases"
  key_links:
    - from: "src/dataset_generator/generation/orchestrator.py"
      to: "src/dataset_generator/generation/generators/deepeval_gen.py"
      via: "calls generate_with_deepeval when function calling selects it"
      pattern: "generate_with_deepeval"
    - from: "src/dataset_generator/generation/orchestrator.py"
      to: "src/dataset_generator/generation/fallback.py"
      via: "catches framework errors and calls fallback"
      pattern: "generate_with_openai_fallback"
    - from: "src/dataset_generator/generation/orchestrator.py"
      to: "src/dataset_generator/generation/adapters/deepeval_adapter.py"
      via: "adapts framework output to Pydantic models"
      pattern: "adapt_deepeval"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/generation/orchestrator.py"
      via: "calls orchestrate_generation for each use case"
      pattern: "orchestrate_generation"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/models/run_manifest.py"
      via: "creates and writes RunManifest at end of pipeline"
      pattern: "RunManifest"
---

<objective>
Build the OpenAI function-calling orchestrator that routes between frameworks, add coverage enforcement, and wire everything into the existing pipeline and CLI to produce test_cases.json, dataset.json, and run_manifest.json.

Purpose: This plan connects all the generation pieces (generators + adapters from Plan 03-02) into a working orchestration layer and integrates it into the existing CLI pipeline. The orchestrator uses OpenAI function calling to decide which framework to invoke (per user locked decision), coverage enforcement ensures minimum quality thresholds, and the pipeline produces all required output artifacts.

Output: Working orchestrator with function calling, coverage enforcer, updated pipeline producing test_cases.json + dataset.json + run_manifest.json, and updated CLI.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-test-dataset-generation/03-RESEARCH.md
@.planning/phases/03-test-dataset-generation/03-CONTEXT.md
@.planning/phases/03-test-dataset-generation/03-01-SUMMARY.md
@.planning/phases/03-test-dataset-generation/03-02-SUMMARY.md

@src/dataset_generator/generation/orchestrator.py
@src/dataset_generator/generation/generators/deepeval_gen.py
@src/dataset_generator/generation/generators/ragas_gen.py
@src/dataset_generator/generation/generators/giskard_gen.py
@src/dataset_generator/generation/adapters/deepeval_adapter.py
@src/dataset_generator/generation/adapters/ragas_adapter.py
@src/dataset_generator/generation/adapters/giskard_adapter.py
@src/dataset_generator/generation/fallback.py
@src/dataset_generator/pipeline.py
@src/dataset_generator/cli.py
@src/dataset_generator/models/test_case.py
@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/models/run_manifest.py
@src/dataset_generator/utils/file_writer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI function-calling orchestrator and coverage enforcer</name>
  <files>
    src/dataset_generator/generation/orchestrator.py
    src/dataset_generator/generation/coverage.py
  </files>
  <action>
**File: src/dataset_generator/generation/orchestrator.py**

Create the central orchestration module that uses OpenAI function calling to route between frameworks (per user locked decision: "OpenAI function calling routes between frameworks, not a fixed pipeline").

```python
def orchestrate_generation(
    use_case: "UseCase",
    policies: list["Policy"],
    document_path: str,
    model: str = "gpt-4o-mini",
    seed: int | None = None,
    min_test_cases: int = 3,
) -> tuple[list["TestCase"], list["DatasetExample"]]:
```

Implementation:

1. **Define tool specifications** for OpenAI function calling (3 tools):

   Tool 1: `generate_with_deepeval`
   - description: "Generate test cases and dataset examples from documents using evolution techniques. Use when: generating from policy documents, need diverse question types, bulk generation required."
   - parameters: document_paths (array of strings), num_goldens (integer, default 10), include_expected_output (boolean, default true)
   - strict: true

   Tool 2: `generate_with_ragas`
   - description: "Generate RAG-specific test questions with knowledge graph transformations. Use when: need multi-context questions, reasoning questions, evaluating retrieval quality."
   - parameters: test_size (integer, default 10), reasoning_ratio (number, default 0.4)
   - strict: true

   Tool 3: `generate_with_giskard`
   - description: "Generate business tests from knowledge base with component evaluation. Use when: need knowledge base validation, business test scenarios."
   - parameters: num_questions (integer, default 20)
   - strict: true

2. **Build orchestration messages:**
   - system: "You are a test generation orchestrator. Given a use case and its policies, select the appropriate framework(s) to generate test cases and dataset examples. Consider the nature of the content when choosing: document-heavy content benefits from DeepEval evolution, multi-context reasoning benefits from Ragas, and knowledge base validation benefits from Giskard. You may call multiple tools."
   - user: formatted message with use_case.name, use_case.description, number of policies, policy summaries (first 200 chars each), document_path

3. **Call OpenAI with tools:**
   ```python
   client = get_openai_client()
   response = client.chat.completions.create(
       model=model,  # Use orchestrator model (could be gpt-4o for better routing)
       messages=messages,
       tools=tools,
       tool_choice="auto",
       temperature=0,
       seed=seed,
   )
   ```

4. **Process tool calls and route to generators:**
   - For each tool_call in response.choices[0].message.tool_calls:
     - Parse arguments
     - Call the appropriate generator function
     - If generator raises an exception: log warning, skip to next tool call
     - If generator succeeds: adapt results using the corresponding adapter
   - Collect all TestCase and DatasetExample objects

5. **Fallback logic (per user locked decision):**
   - If ALL tool calls failed (or no tool calls returned): call generate_with_openai_fallback
   - If total test_cases < min_test_cases after all tool calls: call fallback to generate remaining needed

6. **Track frameworks_used:** Keep a list of which frameworks successfully produced results (for run_manifest)

7. Return (test_cases, dataset_examples)

Also create a helper:
```python
def prepare_policy_documents(policies: list["Policy"], document_path: str) -> str:
    """Prepare a temporary document from policies for framework consumption.
    Writes policy descriptions to a temp file that generators can read.
    Returns path to temp file."""
```

This is needed because DeepEval/Ragas expect file paths, but we have in-memory policy objects.

**File: src/dataset_generator/generation/coverage.py**

Create coverage enforcement (per PIPE-04 requirements):

```python
def enforce_coverage(
    use_case_id: str,
    test_cases: list["TestCase"],
    examples: list["DatasetExample"],
    min_test_cases: int = 3,
    min_criteria: int = 3,
    min_policy_ids: int = 1,
) -> tuple[list["TestCase"], list["DatasetExample"], list[str]]:
    """Enforce minimum coverage requirements.

    Returns:
        Tuple of (valid_test_cases, valid_examples, warnings)
        Raises ValueError if minimum test case count not met.
    """
```

Implementation:
1. Check test_cases count >= min_test_cases, raise ValueError if not
2. For each test_case: verify 2-3 parameter_variation_axes (already enforced by model, but log)
3. For each example: verify evaluation_criteria >= min_criteria (already enforced by model, but log)
4. For each example: verify policy_ids >= min_policy_ids (already enforced by model, but log)
5. Verify each test_case has at least 1 associated example (match by test_case_id)
6. Return validated lists and any warning messages

Also create:
```python
def check_referential_integrity(
    use_cases: list["UseCase"],
    test_cases: list["TestCase"],
    examples: list["DatasetExample"],
) -> list[str]:
    """Check all IDs reference valid parents.
    Returns list of integrity violation messages."""
```
- Every test_case.use_case_id must match a use_case.id
- Every example.test_case_id must match a test_case.id
- Every example.use_case_id must match a use_case.id
- Every example.policy_ids item must start with pol_ (format check only, since policies may come from extraction)
  </action>
  <verify>
1. `python -c "from dataset_generator.generation.orchestrator import orchestrate_generation; print('Orchestrator OK')"` -- must succeed
2. `python -c "from dataset_generator.generation.coverage import enforce_coverage, check_referential_integrity; print('Coverage OK')"` -- must succeed
3. Test coverage enforcer:
`python -c "
from dataset_generator.generation.coverage import enforce_coverage
from dataset_generator.models.test_case import TestCase
from dataset_generator.models.dataset_example import DatasetExample, InputData, Message
tcs = [TestCase(id=f'tc_00{i}', use_case_id='uc_001', name=f'TC{i}', description='D', parameter_variation_axes=['a','b']) for i in range(1,4)]
exs = [DatasetExample(id=f'ex_00{i}', case='test', format='qa', use_case_id='uc_001', test_case_id=f'tc_00{i}', input=InputData(messages=[Message(role='user', content='Q')]), expected_output='A', evaluation_criteria=['c1','c2','c3'], policy_ids=['pol_001']) for i in range(1,4)]
valid_tcs, valid_exs, warnings = enforce_coverage('uc_001', tcs, exs)
assert len(valid_tcs) == 3
print('Coverage test passed')
"` -- must print "Coverage test passed"
  </verify>
  <done>
Orchestrator uses OpenAI function calling with 3 tool definitions to route between DeepEval, Ragas, and Giskard based on task context. Fallback to direct OpenAI generation triggers on framework failure or insufficient results. Coverage enforcer validates minimum 3 test cases per use case, 3+ evaluation criteria, 1+ policy_ids, and referential integrity between use_case_id and test_case_id chains.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire orchestrator into pipeline, add run_manifest output, and update CLI</name>
  <files>
    src/dataset_generator/pipeline.py
    src/dataset_generator/cli.py
    src/dataset_generator/utils/file_writer.py
  </files>
  <action>
**File: src/dataset_generator/pipeline.py**

Extend the existing pipeline to include test case and dataset example generation. The current pipeline has 7 steps (parse -> extract UCs -> extract policies -> validate evidence -> write use_cases.json -> write policies.json -> summary). Add new steps after policy extraction.

Update `PipelineResult` dataclass to include new fields:
- test_cases_path: Path
- dataset_path: Path
- manifest_path: Path
- test_case_count: int
- dataset_example_count: int
- frameworks_used: list[str]

Update `run_pipeline()` to add new steps:

**New Step 5: Generate test cases and dataset examples**
```python
from .generation.orchestrator import orchestrate_generation
from .generation.coverage import enforce_coverage, check_referential_integrity
from .models import TestCaseList, DatasetExampleList, RunManifest, LLMConfig, GenerationCounts

all_test_cases = []
all_examples = []
frameworks_used = set()

for use_case in use_case_list.use_cases:
    # Get policies relevant to this use case (all policies for now; Phase 4 may add filtering)
    test_cases, examples = orchestrate_generation(
        use_case=use_case,
        policies=policy_list.policies,
        document_path=str(config.input_file),
        model=config.model,
        seed=config.seed,
        min_test_cases=config.n_test_cases_per_uc,
    )

    # Enforce coverage per use case
    valid_tcs, valid_exs, warnings = enforce_coverage(
        use_case.id, test_cases, examples,
        min_test_cases=config.n_test_cases_per_uc,
    )

    all_test_cases.extend(valid_tcs)
    all_examples.extend(valid_exs)

    # Track which frameworks were used (from metadata)
    for tc in valid_tcs:
        if "generator" in tc.metadata:
            frameworks_used.add(tc.metadata["generator"])

    for warning in warnings:
        logger.warning(warning)
```

**New Step 6: Check referential integrity**
```python
integrity_issues = check_referential_integrity(
    use_case_list.use_cases, all_test_cases, all_examples
)
for issue in integrity_issues:
    logger.warning(f"Integrity: {issue}")
```

**New Step 7: Write test_cases.json**
```python
test_cases_list = TestCaseList(test_cases=all_test_cases)
test_cases_path = config.out_dir / "test_cases.json"
write_json_output(test_cases_list, test_cases_path)
```

**New Step 8: Write dataset.json**
```python
dataset_list = DatasetExampleList(examples=all_examples)
dataset_path = config.out_dir / "dataset.json"
write_json_output(dataset_list, dataset_path)
```

**Update existing Step (write policies/use_cases): Keep as-is, just renumber.**

**New Step 9: Write run_manifest.json (DATA-08)**
```python
from datetime import datetime, timezone
manifest = RunManifest(
    input_path=str(config.input_file),
    out_path=str(config.out_dir),
    seed=config.seed,
    timestamp=datetime.now(timezone.utc).isoformat(),
    generator_version=__version__,
    llm=LLMConfig(provider="openai", model=config.model, temperature=0.0),
    frameworks_used=sorted(frameworks_used),
    counts=GenerationCounts(
        use_cases=len(use_case_list.use_cases),
        policies=len(policy_list.policies),
        test_cases=len(all_test_cases),
        dataset_examples=len(all_examples),
    ),
)
manifest_path = config.out_dir / "run_manifest.json"
write_json_output(manifest, manifest_path)
```

**Update summary print:** Add test case and dataset example counts, frameworks used.

**File: src/dataset_generator/cli.py**

Update the generate command's result handling to display new outputs:
- Print test_case_count and dataset_example_count
- Print test_cases.json and dataset.json paths
- Print run_manifest.json path
- Print which frameworks were used

**File: src/dataset_generator/utils/file_writer.py**

No structural changes needed (write_json_output already handles any Pydantic BaseModel). But verify it works with nested models (RunManifest has nested LLMConfig and GenerationCounts). If model_dump() doesn't recursively serialize, fix by adding `mode='python'` or explicit dict conversion.
  </action>
  <verify>
1. `python -c "from dataset_generator.pipeline import run_pipeline, PipelineConfig, PipelineResult; print('Pipeline OK')"` -- must succeed
2. `python -m dataset_generator generate --help` -- must show all options and succeed
3. Verify PipelineResult has new fields: `python -c "from dataset_generator.pipeline import PipelineResult; import inspect; sig = inspect.signature(PipelineResult); assert 'test_cases_path' in [p for p in sig.parameters]; print('PipelineResult updated')"` -- or check the dataclass fields directly
4. Dry-run verification (without OPENAI_API_KEY): `python -c "from dataset_generator.generation.orchestrator import orchestrate_generation; from dataset_generator.generation.coverage import enforce_coverage; from dataset_generator.models import RunManifest, LLMConfig, GenerationCounts; print('All wired OK')"` -- must succeed
  </verify>
  <done>
Pipeline extended with test case generation, dataset example generation, coverage enforcement, referential integrity checks, and run_manifest.json output. CLI displays all new output paths and counts. Pipeline produces 4 output files: use_cases.json, policies.json, test_cases.json, dataset.json, plus run_manifest.json per run (DATA-08). OpenAI function-calling orchestrator invoked per use case with fallback on failure.
  </done>
</task>

</tasks>

<verification>
1. Orchestrator imports and has tool definitions for 3 frameworks
2. Coverage enforcer validates minimum thresholds and referential integrity
3. Pipeline produces test_cases.json, dataset.json, and run_manifest.json
4. run_manifest.json contains all DATA-08 required fields
5. CLI shows new output paths
6. Fallback mechanism exists for framework failures
7. Frameworks used are tracked in run_manifest
</verification>

<success_criteria>
- OpenAI function calling orchestrates framework selection (not a fixed pipeline)
- Coverage enforcement ensures minimum 3 test cases per UC, 3+ eval criteria per example
- Pipeline produces use_cases.json + policies.json + test_cases.json + dataset.json + run_manifest.json
- run_manifest.json matches DATA-08 spec (input_path, out_path, seed, timestamp, generator_version, llm block)
- Framework failures trigger fallback to direct OpenAI generation
- All generated items have "generator" field in metadata
</success_criteria>

<output>
After completion, create `.planning/phases/03-test-dataset-generation/03-03-SUMMARY.md`
</output>
