---
phase: 03-test-dataset-generation
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/dataset_generator/generation/__init__.py
  - src/dataset_generator/generation/generators/__init__.py
  - src/dataset_generator/generation/generators/deepeval_gen.py
  - src/dataset_generator/generation/generators/ragas_gen.py
  - src/dataset_generator/generation/generators/giskard_gen.py
  - src/dataset_generator/generation/adapters/__init__.py
  - src/dataset_generator/generation/adapters/deepeval_adapter.py
  - src/dataset_generator/generation/adapters/ragas_adapter.py
  - src/dataset_generator/generation/adapters/giskard_adapter.py
  - src/dataset_generator/generation/fallback.py
autonomous: true

must_haves:
  truths:
    - "DeepEval generator produces goldens from policy documents using Synthesizer with evolution and filtration"
    - "Ragas generator produces RAG-specific test questions with question type distributions (simple, reasoning, multi-context)"
    - "Giskard generator produces knowledge-base-derived test questions using RAGET"
    - "Each adapter converts framework-native output to project TestCase and DatasetExample Pydantic models"
    - "Each adapted output includes generator field in metadata tracking which framework produced it"
    - "Fallback generator uses direct OpenAI structured outputs when a framework call fails"
  artifacts:
    - path: "src/dataset_generator/generation/generators/deepeval_gen.py"
      provides: "DeepEval Synthesizer wrapper generating goldens from documents with evolution techniques"
      contains: "Synthesizer"
    - path: "src/dataset_generator/generation/generators/ragas_gen.py"
      provides: "Ragas TestsetGenerator wrapper producing RAG evaluation questions"
      contains: "TestsetGenerator"
    - path: "src/dataset_generator/generation/generators/giskard_gen.py"
      provides: "Giskard RAGET wrapper generating knowledge-base questions"
      contains: "KnowledgeBase"
    - path: "src/dataset_generator/generation/adapters/deepeval_adapter.py"
      provides: "Hardcoded adapter converting DeepEval goldens to TestCase + DatasetExample"
      contains: "generator.*deepeval"
    - path: "src/dataset_generator/generation/adapters/ragas_adapter.py"
      provides: "Hardcoded adapter converting Ragas testset rows to TestCase + DatasetExample"
      contains: "generator.*ragas"
    - path: "src/dataset_generator/generation/adapters/giskard_adapter.py"
      provides: "Hardcoded adapter converting Giskard testset rows to TestCase + DatasetExample"
      contains: "generator.*giskard"
    - path: "src/dataset_generator/generation/fallback.py"
      provides: "Direct OpenAI generation fallback for when frameworks fail"
      contains: "fallback"
  key_links:
    - from: "src/dataset_generator/generation/adapters/deepeval_adapter.py"
      to: "src/dataset_generator/models/test_case.py"
      via: "imports TestCase to construct adapted output"
      pattern: "from.*models.*import.*TestCase"
    - from: "src/dataset_generator/generation/adapters/deepeval_adapter.py"
      to: "src/dataset_generator/models/dataset_example.py"
      via: "imports DatasetExample, Message, InputData to construct adapted output"
      pattern: "from.*models.*import.*DatasetExample"
    - from: "src/dataset_generator/generation/fallback.py"
      to: "src/dataset_generator/extraction/llm_client.py"
      via: "uses OpenAI client for direct generation"
      pattern: "from.*llm_client import"
---

<objective>
Implement all three framework generators (DeepEval, Ragas, Giskard), their hardcoded Pydantic adapters, and a direct OpenAI fallback generator.

Purpose: This plan builds the generation layer -- the concrete implementations that produce test cases and dataset examples from each framework. The adapters ensure all framework-native outputs are converted to project Pydantic contracts with the `generator` metadata field for traceability. The fallback ensures the pipeline always completes even if a framework call fails (per user decision).

Output: Three generator modules, three adapter modules, one fallback module, and package init files for the generation directory tree.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-test-dataset-generation/03-RESEARCH.md
@.planning/phases/03-test-dataset-generation/03-CONTEXT.md
@.planning/phases/03-test-dataset-generation/03-01-SUMMARY.md

@src/dataset_generator/models/test_case.py
@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/models/run_manifest.py
@src/dataset_generator/extraction/llm_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DeepEval generator, Ragas generator, Giskard generator, and fallback</name>
  <files>
    src/dataset_generator/generation/__init__.py
    src/dataset_generator/generation/generators/__init__.py
    src/dataset_generator/generation/generators/deepeval_gen.py
    src/dataset_generator/generation/generators/ragas_gen.py
    src/dataset_generator/generation/generators/giskard_gen.py
    src/dataset_generator/generation/fallback.py
  </files>
  <action>
Create the generation package directory structure first:
```
src/dataset_generator/generation/
  __init__.py
  generators/
    __init__.py
    deepeval_gen.py
    ragas_gen.py
    giskard_gen.py
  fallback.py
```

**File: src/dataset_generator/generation/generators/deepeval_gen.py**

DeepEval is the PRIMARY generation engine (per user locked decision). Create a `generate_with_deepeval()` function:

```python
def generate_with_deepeval(
    document_paths: list[str],
    num_goldens: int = 10,
    include_expected_output: bool = True,
    model: str = "gpt-4o-mini",
) -> list:  # Returns list of DeepEval Golden objects
```

Implementation:
1. Import `Synthesizer`, `Evolution` from `deepeval.synthesizer` and `EvolutionConfig`, `FiltrationConfig`, `StylingConfig` from `deepeval.synthesizer.config`
2. Configure evolution for diverse test case generation (per research Pattern 2):
   - Evolution.REASONING: 0.25 (why does policy X apply?)
   - Evolution.MULTICONTEXT: 0.25 (combine multiple policy sections)
   - Evolution.CONCRETIZING: 0.25 (specific scenarios)
   - Evolution.CONSTRAINED: 0.25 (edge cases with limitations)
   - num_evolutions=2
3. Configure filtration: critic_model="gpt-4o", quality threshold=0.7, max_quality_retries=3
4. Configure styling for customer support context:
   - input_format: "Customer support message asking about specific policy or procedure"
   - expected_output_format: "Support response referencing relevant policies"
   - task: "Customer support query handling"
5. Create Synthesizer instance with model=model, async_mode=False (simpler for initial implementation; async can be added later)
6. Call `synthesizer.generate_goldens_from_docs(document_paths=document_paths, include_expected_output=include_expected_output)`
7. Return the list of goldens
8. Wrap entire body in try/except, log errors, re-raise as RuntimeError with descriptive message

IMPORTANT: If the DeepEval Synthesizer API has changed from what the research shows, adapt accordingly. The key is: use Synthesizer to generate goldens from document paths with evolution techniques. Check the actual installed version's API.

**File: src/dataset_generator/generation/generators/ragas_gen.py**

Ragas role: RAG-specific question generation with knowledge graph transformations (per research recommendation). Create:

```python
def generate_with_ragas(
    document_paths: list[str],
    test_size: int = 10,
    reasoning_ratio: float = 0.4,
    model: str = "gpt-4o-mini",
) -> "pandas.DataFrame":  # Returns Ragas testset as DataFrame
```

Implementation:
1. Load documents using langchain document loaders (TextLoader or similar for markdown)
2. Initialize TestsetGenerator with OpenAI (generator = TestsetGenerator.with_openai())
3. Configure distributions: simple = 1 - reasoning_ratio - 0.2, reasoning = reasoning_ratio, multi_context = 0.2
4. Call generator.generate_with_langchain_docs(documents=docs, test_size=test_size, distributions=distributions)
5. Return testset.to_pandas()
6. Wrap in try/except, log errors, re-raise as RuntimeError

NOTE: The Ragas API may differ from research examples (v0.4.x may have updated APIs). Check actual installed API and adapt. The core capability needed: generate test questions from documents with question type distribution control.

**File: src/dataset_generator/generation/generators/giskard_gen.py**

Giskard role: Knowledge base validation and document-based business test generation (per research recommendation). Create:

```python
def generate_with_giskard(
    knowledge_base_df: "pandas.DataFrame",
    num_questions: int = 20,
    agent_description: str = "Customer support agent with policy knowledge base access",
    language: str = "ru",
) -> "pandas.DataFrame":  # Returns Giskard testset as DataFrame
```

Implementation:
1. Import KnowledgeBase, generate_testset from giskard.rag
2. Create KB from DataFrame: kb = KnowledgeBase.from_pandas(knowledge_base_df, columns=["content"])
3. Generate: testset = generate_testset(kb, num_questions=num_questions, language=language, agent_description=agent_description)
4. Return testset.to_pandas()
5. Wrap in try/except, log errors, re-raise as RuntimeError

NOTE: Giskard is SLOW (15+ min for 60 questions per research pitfall 3). Start with num_questions=20 default. Cache knowledge base if possible.

**File: src/dataset_generator/generation/fallback.py**

Direct OpenAI generation for when frameworks fail (per user locked decision). Create:

```python
def generate_with_openai_fallback(
    use_case_id: str,
    use_case_description: str,
    policies: list[dict],
    num_test_cases: int = 3,
    model: str = "gpt-4o-mini",
    seed: int | None = None,
) -> tuple[list["TestCase"], list["DatasetExample"]]:
```

Implementation:
1. Import get_openai_client from extraction.llm_client
2. Build a system prompt that describes the test case generation task, including:
   - The use case description and ID
   - The policies (id, name, type, description)
   - Instructions to generate test cases with 2-3 parameter variation axes
   - Instructions to generate dataset examples with input messages, expected_output, evaluation_criteria (3+), policy_ids
   - Output format matching TestCase and DatasetExample schemas
3. Use client.chat.completions.create with response_format={"type": "json_object"} (or structured outputs if available)
4. Parse JSON response and construct TestCase and DatasetExample Pydantic objects manually
5. Set metadata.generator = "openai_fallback" on all generated items
6. Use temperature=0 and pass seed for reproducibility

This is the LAST RESORT generator. It should work without any external framework dependencies.

**Init files:** Create __init__.py for generation/ and generation/generators/ that import key functions.
  </action>
  <verify>
1. `python -c "from dataset_generator.generation.generators.deepeval_gen import generate_with_deepeval; print('DeepEval gen OK')"` -- must succeed
2. `python -c "from dataset_generator.generation.generators.ragas_gen import generate_with_ragas; print('Ragas gen OK')"` -- must succeed
3. `python -c "from dataset_generator.generation.generators.giskard_gen import generate_with_giskard; print('Giskard gen OK')"` -- must succeed
4. `python -c "from dataset_generator.generation.fallback import generate_with_openai_fallback; print('Fallback OK')"` -- must succeed
  </verify>
  <done>
All four generators importable: deepeval_gen (Synthesizer with evolution), ragas_gen (TestsetGenerator with distributions), giskard_gen (RAGET from knowledge base), and fallback (direct OpenAI structured output). Each wraps framework calls in try/except and raises RuntimeError on failure.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create hardcoded adapters for DeepEval, Ragas, and Giskard</name>
  <files>
    src/dataset_generator/generation/adapters/__init__.py
    src/dataset_generator/generation/adapters/deepeval_adapter.py
    src/dataset_generator/generation/adapters/ragas_adapter.py
    src/dataset_generator/generation/adapters/giskard_adapter.py
  </files>
  <action>
Create the adapters directory and three hardcoded adapter modules. Per user locked decision: "Hardcoded Python adapters convert between framework output formats and Pydantic data contracts (not LLM-driven conversion)."

**File: src/dataset_generator/generation/adapters/deepeval_adapter.py**

Create two functions:

```python
def adapt_deepeval_golden_to_test_case(
    golden,  # DeepEval Golden object
    use_case_id: str,
    test_case_index: int,
) -> "TestCase":
```
- Generate tc_id as f"tc_{use_case_id.replace('uc_', '')}_{test_case_index:03d}"
- Extract name from golden.input (first 80 chars or meaningful summary)
- description from golden.context or golden.input
- parameter_variation_axes: derive from golden metadata or default to ["tone", "complexity", "policy_boundary"] (must be 2-3 items)
- metadata = {"generator": "deepeval", "evolution_type": golden.additional_metadata.get("evolution_type"), "quality_score": golden.additional_metadata.get("quality_score")}

```python
def adapt_deepeval_golden_to_example(
    golden,  # DeepEval Golden object
    use_case_id: str,
    test_case_id: str,
    example_index: int,
    case: str = "support_bot",
    format: str = "single_turn_qa",
    policy_ids: list[str] | None = None,
) -> "DatasetExample":
```
- Generate ex_id as f"ex_{use_case_id.replace('uc_', '')}_{example_index:03d}"
- input = InputData(messages=[Message(role="user", content=golden.input)])
- expected_output from golden.expected_output or ""
- evaluation_criteria: extract from golden context or generate defaults ["relevance_to_query", "policy_compliance", "response_completeness"] (must be 3+)
- policy_ids from parameter or extract from golden.context using regex for pol_ pattern, default to ["pol_unknown"] if empty
- metadata = {"generator": "deepeval", ...preserve raw golden metadata}

Handle null/missing fields gracefully with defaults and logging.warning() per research pitfall 5.

**File: src/dataset_generator/generation/adapters/ragas_adapter.py**

Same two-function pattern (adapt_ragas_row_to_test_case and adapt_ragas_row_to_example):
- Input: pandas DataFrame row (from testset.to_pandas())
- Access row fields: row.question, row.ground_truth, row.contexts, row.evolution_type, row.metadata
- For test_case: parameter_variation_axes based on evolution_type (reasoning -> ["reasoning_depth", "context_complexity"], multi_context -> ["context_count", "cross_reference_depth"], simple -> ["tone", "specificity"])
- metadata = {"generator": "ragas", "evolution_type": row.evolution_type, "contexts_used": len(row.contexts) if hasattr}
- Handle NaN values for ground_truth (per research pitfall 2: "Occasional NaN values for ground_truth")

**File: src/dataset_generator/generation/adapters/giskard_adapter.py**

Same two-function pattern (adapt_giskard_row_to_test_case and adapt_giskard_row_to_example):
- Input: pandas DataFrame row (from testset.to_pandas())
- Access row fields: row.question, row.reference_answer, row.reference_context, row.metadata
- For test_case: parameter_variation_axes based on question_type from metadata
- metadata = {"generator": "giskard", "question_type": row.metadata.get("question_type") if hasattr, "reference_context": str(row.reference_context)[:200]}
- Handle missing reference_answer (use "" as default)

**All adapters must:**
1. Return valid Pydantic model instances (TestCase or DatasetExample)
2. Include "generator" key in metadata dict
3. Handle missing/null fields with sensible defaults and logging.warning
4. Never call an LLM -- pure Python field mapping only
5. Be importable and usable without framework being installed (accept generic objects)

**Init file:** src/dataset_generator/generation/adapters/__init__.py imports all adapter functions.
  </action>
  <verify>
1. `python -c "
from dataset_generator.generation.adapters.deepeval_adapter import adapt_deepeval_golden_to_test_case, adapt_deepeval_golden_to_example
from dataset_generator.generation.adapters.ragas_adapter import adapt_ragas_row_to_test_case, adapt_ragas_row_to_example
from dataset_generator.generation.adapters.giskard_adapter import adapt_giskard_row_to_test_case, adapt_giskard_row_to_example
print('All adapters importable')
"` -- must succeed
2. Test DeepEval adapter with mock object:
`python -c "
from types import SimpleNamespace
from dataset_generator.generation.adapters.deepeval_adapter import adapt_deepeval_golden_to_test_case, adapt_deepeval_golden_to_example
mock_golden = SimpleNamespace(input='Test question', expected_output='Test answer', context=['Policy context'], additional_metadata={'evolution_type': 'reasoning', 'quality_score': 0.85})
tc = adapt_deepeval_golden_to_test_case(mock_golden, 'uc_001', 1)
assert tc.id.startswith('tc_')
assert tc.metadata.get('generator') == 'deepeval'
assert 2 <= len(tc.parameter_variation_axes) <= 3
ex = adapt_deepeval_golden_to_example(mock_golden, 'uc_001', tc.id, 1, policy_ids=['pol_001'])
assert ex.id.startswith('ex_')
assert ex.metadata.get('generator') == 'deepeval'
assert len(ex.evaluation_criteria) >= 3
print('DeepEval adapter test passed')
"` -- must print "DeepEval adapter test passed"
  </verify>
  <done>
Three hardcoded adapters (deepeval_adapter.py, ragas_adapter.py, giskard_adapter.py) convert framework-native outputs to project TestCase and DatasetExample models. All adapters include "generator" metadata field, handle null/missing fields with defaults, and use pure Python mapping (no LLM calls). All adapter functions importable and tested with mock objects.
  </done>
</task>

</tasks>

<verification>
1. Generation package structure exists: generation/__init__.py, generators/, adapters/
2. All 4 generators importable (deepeval, ragas, giskard, fallback)
3. All 3 adapters importable with 2 functions each (to_test_case, to_example)
4. Adapters produce valid Pydantic models when given mock input
5. All metadata dicts include "generator" field
6. Fallback uses direct OpenAI client from extraction.llm_client
7. No adapter calls an LLM (hardcoded mapping only)
</verification>

<success_criteria>
- DeepEval generator wraps Synthesizer with evolution/filtration config
- Ragas generator wraps TestsetGenerator with distribution control
- Giskard generator wraps RAGET with knowledge base input
- Fallback generator uses direct OpenAI structured outputs
- Each adapter converts framework output to TestCase + DatasetExample with "generator" metadata
- All code importable without runtime errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-test-dataset-generation/03-02-SUMMARY.md`
</output>
