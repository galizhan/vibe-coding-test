---
phase: 03-test-dataset-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/dataset_generator/models/test_case.py
  - src/dataset_generator/models/dataset_example.py
  - src/dataset_generator/models/run_manifest.py
  - src/dataset_generator/models/__init__.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "TestCase model validates tc_ ID prefix, requires 2-3 parameter_variation_axes, references use_case_id with uc_ prefix"
    - "DatasetExample model validates ex_ ID prefix, requires 3+ evaluation_criteria, 1+ policy_ids with pol_ prefix, input messages with role field"
    - "RunManifest model captures input_path, out_path, seed, timestamp, generator_version, and llm block"
    - "All three external frameworks (deepeval, ragas, giskard) are installable alongside existing dependencies"
  artifacts:
    - path: "src/dataset_generator/models/test_case.py"
      provides: "TestCase and TestCaseList Pydantic v2 models with tc_ prefix validation and 2-3 variation axes constraint"
      contains: "tc_"
    - path: "src/dataset_generator/models/dataset_example.py"
      provides: "DatasetExample and DatasetExampleList Pydantic v2 models with ex_ prefix, 3+ eval criteria, 1+ policy_ids"
      contains: "ex_"
    - path: "src/dataset_generator/models/run_manifest.py"
      provides: "RunManifest Pydantic v2 model with input_path, out_path, seed, timestamp, generator_version, llm block"
      contains: "RunManifest"
    - path: "src/dataset_generator/models/__init__.py"
      provides: "Re-exports all models including new TestCase, DatasetExample, RunManifest"
      contains: "TestCase"
    - path: "pyproject.toml"
      provides: "Framework dependencies: deepeval, ragas, giskard, evidently"
      contains: "deepeval"
  key_links:
    - from: "src/dataset_generator/models/test_case.py"
      to: "src/dataset_generator/models/evidence.py"
      via: "imports Evidence model for consistency"
      pattern: "from.*evidence import"
    - from: "src/dataset_generator/models/dataset_example.py"
      to: "src/dataset_generator/models/test_case.py"
      via: "references test_case_id field matching TestCase.id"
      pattern: "test_case_id"
---

<objective>
Define Pydantic v2 data contracts for test cases, dataset examples, and run manifests, and install all external framework dependencies.

Purpose: All downstream generation work (Plans 03-02 through 03-04) depends on having validated data models and installed framework libraries. This plan establishes the schema foundation and dependency baseline for the entire phase.

Output: Three new Pydantic model files (test_case.py, dataset_example.py, run_manifest.py), updated models __init__.py, and pyproject.toml with all framework dependencies installed.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-test-dataset-generation/03-RESEARCH.md
@.planning/phases/03-test-dataset-generation/03-CONTEXT.md

@src/dataset_generator/models/__init__.py
@src/dataset_generator/models/evidence.py
@src/dataset_generator/models/use_case.py
@src/dataset_generator/models/policy.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TestCase and DatasetExample Pydantic v2 models</name>
  <files>
    src/dataset_generator/models/test_case.py
    src/dataset_generator/models/dataset_example.py
  </files>
  <action>
Create two new Pydantic v2 model files following the same patterns as existing models (use_case.py, policy.py).

**File: src/dataset_generator/models/test_case.py**

Create `TestCase` and `TestCaseList` models:
- `id: str` with field_validator enforcing `tc_` prefix (same pattern as uc_ and pol_ validation)
- `use_case_id: str` with field_validator enforcing `uc_` prefix (referential integrity)
- `name: str` - short descriptive name of the test case
- `description: str` - detailed description of what this test case covers
- `parameter_variation_axes: list[str]` with field_validator enforcing 2-3 items (per PIPE-04: "2-3 parameter variation axes per use case"). These are the dimensions along which the test varies (e.g., "tone", "language complexity", "policy boundary").
- `metadata: dict` with Field(default_factory=dict) - includes `generator` field for framework tracking

`TestCaseList` wraps `test_cases: list[TestCase]`

**File: src/dataset_generator/models/dataset_example.py**

Create `DatasetExample` and `DatasetExampleList` models per DATA-07:
- `id: str` with field_validator enforcing `ex_` prefix
- `case: str` - case identifier (e.g., "support_bot", "operator_quality")
- `format: str` - format type (e.g., "single_turn_qa", "single_utterance_correction", "dialog_last_turn_correction")
- `use_case_id: str` with field_validator enforcing `uc_` prefix
- `test_case_id: str` with field_validator enforcing `tc_` prefix
- `input: dict` containing `messages: list[dict]` where each message has `role` (Literal["user", "operator", "assistant", "system"]) and `content` (str). Use a nested `InputMessages` model:
  ```python
  class Message(BaseModel):
      role: Literal["user", "operator", "assistant", "system"]
      content: str

  class InputData(BaseModel):
      messages: list[Message] = Field(..., min_length=1)
  ```
- `expected_output: str` - expected response
- `evaluation_criteria: list[str]` with field_validator enforcing min 3 items (per DATA-07: "evaluation_criteria (3+)")
- `policy_ids: list[str]` with field_validator enforcing min 1 item and each starting with `pol_` (per DATA-07: "policy_ids (1+)")
- `metadata: dict` with Field(default_factory=dict) - includes `generator` field for framework tracking

`DatasetExampleList` wraps `examples: list[DatasetExample]`

IMPORTANT: Use Pydantic v2 syntax throughout (field_validator, not validator; model_dump, not dict). Follow the exact pattern used in evidence.py, use_case.py, and policy.py.
  </action>
  <verify>
Run: `python -c "
from dataset_generator.models.test_case import TestCase, TestCaseList
from dataset_generator.models.dataset_example import DatasetExample, DatasetExampleList, Message, InputData
# Test TestCase validation
tc = TestCase(id='tc_001', use_case_id='uc_001', name='Test', description='Desc', parameter_variation_axes=['tone', 'complexity'])
assert tc.id == 'tc_001'
# Test axes constraint
try:
    TestCase(id='tc_001', use_case_id='uc_001', name='T', description='D', parameter_variation_axes=['one'])
    assert False, 'Should fail with 1 axis'
except ValueError:
    pass
# Test DatasetExample
ex = DatasetExample(
    id='ex_001', case='support_bot', format='single_turn_qa',
    use_case_id='uc_001', test_case_id='tc_001',
    input=InputData(messages=[Message(role='user', content='Hello')]),
    expected_output='Response',
    evaluation_criteria=['c1', 'c2', 'c3'],
    policy_ids=['pol_001']
)
assert ex.id == 'ex_001'
print('All validations passed')
"` -- must print "All validations passed"
  </verify>
  <done>
TestCase model validates tc_ prefix, enforces 2-3 parameter_variation_axes, and references use_case_id with uc_ prefix. DatasetExample model validates ex_ prefix, enforces 3+ evaluation_criteria, 1+ policy_ids with pol_ prefix, and structured input messages with role validation. Both follow existing Pydantic v2 patterns.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create RunManifest model, update models __init__, and install framework dependencies</name>
  <files>
    src/dataset_generator/models/run_manifest.py
    src/dataset_generator/models/__init__.py
    pyproject.toml
  </files>
  <action>
**Step 1: Create src/dataset_generator/models/run_manifest.py**

Create `RunManifest` Pydantic v2 model per DATA-08:
- `input_path: str` - path to input markdown file
- `out_path: str` - output directory path
- `seed: int | None` - random seed used (None if not specified)
- `timestamp: str` - ISO 8601 timestamp of the run
- `generator_version: str` - version of the generator (from __init__.__version__)
- `llm: LLMConfig` - nested model with:
  ```python
  class LLMConfig(BaseModel):
      provider: str = "openai"
      model: str
      temperature: float = 0.0
  ```
- `frameworks_used: list[str]` - which frameworks were invoked (e.g., ["deepeval", "ragas"])
- `counts: GenerationCounts` - nested model with:
  ```python
  class GenerationCounts(BaseModel):
      use_cases: int = 0
      policies: int = 0
      test_cases: int = 0
      dataset_examples: int = 0
  ```

**Step 2: Update src/dataset_generator/models/__init__.py**

Add imports and exports for all new models: TestCase, TestCaseList, DatasetExample, DatasetExampleList, Message, InputData, RunManifest, LLMConfig, GenerationCounts.

**Step 3: Update pyproject.toml dependencies**

Add the following to the `dependencies` list:
- `"deepeval>=3.0"` (DeepEval Synthesizer - primary generation engine)
- `"ragas>=0.4"` (RAG evaluation and test generation)
- `"giskard[llm]>=2.0"` (Knowledge base testing, RAGET)
- `"evidently>=0.4"` (Data quality reports - INTG-03)
- `"langchain>=0.2"` (Document loading for Ragas/Giskard integration)
- `"pandas>=2.0"` (DataFrame operations for framework exports)

Then run `pip install -e .` to install all dependencies.

NOTE: If any dependency conflicts arise during installation, try relaxing version constraints (remove lower bounds) or use `pip install -e . --no-deps` and install conflicting packages separately. The goal is all frameworks importable. Record any version pins needed in the SUMMARY.
  </action>
  <verify>
1. `python -c "from dataset_generator.models.run_manifest import RunManifest, LLMConfig, GenerationCounts; print('RunManifest OK')"` -- must succeed
2. `python -c "from dataset_generator.models import TestCase, DatasetExample, RunManifest; print('All models importable')"` -- must succeed
3. `pip install -e . && python -c "import deepeval; import ragas; import giskard; import evidently; print('All frameworks installed')"` -- must succeed (may take a few minutes to install)
  </verify>
  <done>
RunManifest model captures all DATA-08 fields (input_path, out_path, seed, timestamp, generator_version, llm config, frameworks_used, generation counts). Models __init__.py re-exports all new types. pyproject.toml includes deepeval, ragas, giskard, evidently, langchain, and pandas. All dependencies installed and importable.
  </done>
</task>

</tasks>

<verification>
1. All 3 new model files exist and are importable
2. TestCase enforces tc_ prefix and 2-3 variation axes
3. DatasetExample enforces ex_ prefix, 3+ evaluation_criteria, 1+ policy_ids with pol_ prefix, role-validated messages
4. RunManifest captures all DATA-08 required fields
5. models/__init__.py exports all new types
6. pyproject.toml has deepeval, ragas, giskard, evidently dependencies
7. All framework imports succeed after `pip install -e .`
</verification>

<success_criteria>
- TestCase, DatasetExample, RunManifest models defined with Pydantic v2 and all field validations passing
- All new models importable from dataset_generator.models
- deepeval, ragas, giskard, evidently, langchain, pandas all installable and importable
- Existing models (Evidence, UseCase, Policy) still work unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/03-test-dataset-generation/03-01-SUMMARY.md`
</output>
