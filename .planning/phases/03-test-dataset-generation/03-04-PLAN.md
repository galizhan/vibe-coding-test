---
phase: 03-test-dataset-generation
plan: 04
type: execute
wave: 4
depends_on: ["03-03"]
files_modified:
  - src/dataset_generator/generation/quality_report.py
  - src/dataset_generator/pipeline.py
autonomous: false

must_haves:
  truths:
    - "Evidently generates data quality reports on the generated dataset (duplicates, distributions, placeholders)"
    - "Pipeline end-to-end produces all required outputs from a markdown document input"
    - "Each use case has minimum 3 test cases with 2-3 parameter variation axes"
    - "Each test case has dataset examples with input messages, expected_output, and 3+ evaluation_criteria"
    - "Dataset examples reference policy_ids and maintain referential integrity"
    - "Generated messages use correct role conventions (user, operator, assistant, system)"
  artifacts:
    - path: "src/dataset_generator/generation/quality_report.py"
      provides: "Evidently-based data quality report generator for dataset analysis"
      contains: "evidently"
    - path: "src/dataset_generator/pipeline.py"
      provides: "Full pipeline with quality report step"
      contains: "quality_report"
  key_links:
    - from: "src/dataset_generator/generation/quality_report.py"
      to: "src/dataset_generator/models/dataset_example.py"
      via: "converts DatasetExample list to DataFrame for Evidently analysis"
      pattern: "DataFrame"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/generation/quality_report.py"
      via: "calls quality report generator after dataset generation"
      pattern: "quality_report"
---

<objective>
Add Evidently data quality reports for generated datasets and verify the complete end-to-end pipeline with a live test run.

Purpose: Evidently integration (INTG-03) provides data quality analysis (duplicates, distributions, placeholder detection) on the generated dataset. The end-to-end verification confirms all Phase 3 success criteria are met: generation coverage, referential integrity, role conventions, and output completeness.

Output: quality_report.py module using Evidently, updated pipeline with quality report step, and verified end-to-end results.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-test-dataset-generation/03-RESEARCH.md
@.planning/phases/03-test-dataset-generation/03-CONTEXT.md
@.planning/phases/03-test-dataset-generation/03-01-SUMMARY.md
@.planning/phases/03-test-dataset-generation/03-02-SUMMARY.md
@.planning/phases/03-test-dataset-generation/03-03-SUMMARY.md

@src/dataset_generator/generation/quality_report.py
@src/dataset_generator/pipeline.py
@src/dataset_generator/models/dataset_example.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Evidently data quality report module and integrate into pipeline</name>
  <files>
    src/dataset_generator/generation/quality_report.py
    src/dataset_generator/pipeline.py
  </files>
  <action>
**File: src/dataset_generator/generation/quality_report.py**

Create a module that uses Evidently to analyze generated dataset quality (INTG-03: "generate data quality reports - duplicates, distributions, placeholders").

```python
def generate_quality_report(
    examples: list["DatasetExample"],
    output_path: "Path",
) -> dict:
    """Generate Evidently data quality report on generated dataset.

    Analyzes:
    - Duplicate detection (near-duplicate input messages)
    - Distribution of cases, formats, evaluation criteria counts
    - Placeholder detection (empty/template content)
    - Policy ID distribution

    Args:
        examples: List of generated DatasetExample objects
        output_path: Directory to write quality_report.html

    Returns:
        Dict with summary metrics: {duplicates, total, distribution, warnings}
    """
```

Implementation:
1. Convert DatasetExample list to pandas DataFrame:
   - Columns: id, case, format, use_case_id, test_case_id, input_text (extracted from input.messages[0].content), expected_output, criteria_count, policy_count, generator
2. Use Evidently to create a data quality report:
   - Import from evidently.report import Report
   - Import from evidently.metric_preset import DataQualityPreset (or relevant metrics)
   - If DataQualityPreset not available in installed version, use individual metrics:
     - DatasetSummaryMetric (overall stats)
     - ColumnDriftMetric or ColumnSummaryMetric for distributions
   - Create Report with relevant metrics
   - Run report on the DataFrame
   - Save as HTML: report.save_html(str(output_path / "quality_report.html"))
3. Extract summary metrics from report results:
   - Total examples count
   - Duplicate count (exact duplicate input_text values)
   - Distribution of "case" and "format" columns
   - Average criteria_count and policy_count
   - Placeholder detection: count examples where expected_output is empty, very short (<10 chars), or contains placeholder patterns like "TODO", "TBD", "placeholder"
4. Return summary dict and print key findings to logger
5. Wrap in try/except: if Evidently fails, log warning and return minimal summary (don't block pipeline)

NOTE: The Evidently API has changed across versions. Check the actually installed version and adapt. The key goal: produce an HTML report analyzing dataset quality. If the API doesn't match exactly, adapt to what's available. At minimum, produce a pandas-based analysis even without Evidently if it fails.

**File: src/dataset_generator/pipeline.py**

Add a new step AFTER dataset.json is written and BEFORE run_manifest:

```python
# Step N: Generate quality report (INTG-03)
logger.info("Step N: Generating data quality report")
try:
    from .generation.quality_report import generate_quality_report
    quality_summary = generate_quality_report(all_examples, config.out_dir)
    logger.info(f"Quality report: {quality_summary.get('total', 0)} examples analyzed, "
                f"{quality_summary.get('duplicates', 0)} potential duplicates")
except Exception as e:
    logger.warning(f"Quality report generation failed (non-blocking): {e}")
    quality_summary = {}
```

Update the summary print to include quality report path if it was generated.
  </action>
  <verify>
1. `python -c "from dataset_generator.generation.quality_report import generate_quality_report; print('Quality report OK')"` -- must succeed
2. Test with mock data:
`python -c "
from dataset_generator.generation.quality_report import generate_quality_report
from dataset_generator.models.dataset_example import DatasetExample, InputData, Message
from pathlib import Path
import tempfile
exs = [
    DatasetExample(
        id=f'ex_00{i}', case='test', format='qa', use_case_id='uc_001',
        test_case_id=f'tc_00{i}', input=InputData(messages=[Message(role='user', content=f'Question {i}')]),
        expected_output=f'Answer {i}', evaluation_criteria=['c1','c2','c3'], policy_ids=['pol_001'],
        metadata={'generator': 'test'}
    ) for i in range(1, 6)
]
with tempfile.TemporaryDirectory() as tmpdir:
    result = generate_quality_report(exs, Path(tmpdir))
    print(f'Report generated: {result}')
"` -- must print report summary without errors
  </verify>
  <done>
Evidently-based quality report module generates data quality analysis (duplicates, distributions, placeholder detection) as HTML report. Pipeline integrates quality report as non-blocking step after dataset generation. Report covers INTG-03 requirements.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end Phase 3 verification</name>
  <files>n/a</files>
  <action>
Run the complete pipeline on the support FAQ example document and verify all Phase 3 success criteria are met. This is a human verification checkpoint -- the user runs the pipeline and inspects outputs.
  </action>
  <verify>
1. Run the full pipeline on the support FAQ example:
   ```bash
   python -m dataset_generator generate \
     --input example_input_raw_support_faq_and_tickets.md \
     --out /tmp/verify_03 \
     --seed 42 --model gpt-4o-mini \
     --n-test-cases-per-uc 3
   ```

2. Check output files exist:
   ```bash
   ls -la /tmp/verify_03/
   ```
   Expected: use_cases.json, policies.json, test_cases.json, dataset.json, run_manifest.json, quality_report.html (or at least 5 of 6 if Evidently report is non-blocking)

3. Verify test_cases.json:
   - All IDs start with tc_
   - Each has 2-3 parameter_variation_axes
   - Each references a valid uc_ use_case_id

4. Verify dataset.json:
   - All IDs start with ex_
   - Each has 3+ evaluation_criteria
   - Each has 1+ policy_ids starting with pol_
   - Input messages have valid roles (user, operator, assistant, system)

5. Verify run_manifest.json:
   - Contains input_path, out_path, seed, timestamp, generator_version, llm block
   - frameworks_used lists at least one framework

6. Check framework tracking: verify at least one example has "generator" in metadata and it is not all "openai_fallback"
  </verify>
  <done>
Pipeline produces all required output files. Test cases have 2-3 variation axes per use case. Dataset examples have 3+ evaluation criteria and 1+ policy_ids. Run manifest captures all metadata. At least one external framework (not just fallback) produced results. User approved the output quality.
  </done>
</task>

</tasks>

<verification>
1. Evidently quality report module importable and produces analysis
2. Pipeline produces all 5-6 output files (use_cases, policies, test_cases, dataset, manifest, report)
3. test_cases.json has tc_ prefixed IDs with 2-3 parameter_variation_axes each
4. dataset.json has ex_ prefixed IDs with 3+ evaluation_criteria and 1+ policy_ids per example
5. run_manifest.json has all DATA-08 fields
6. At least one non-fallback framework produced results (generator != "openai_fallback")
7. Role conventions correct in input messages (user/operator/assistant/system)
</verification>

<success_criteria>
Phase 3 success criteria (from ROADMAP.md):
1. Each use case produces minimum 3 test cases with 2-3 parameter variation axes
2. Each test case produces dataset examples with input messages, expected_output, and evaluation_criteria (3+)
3. Dataset examples reference policy_ids and maintain referential integrity
4. Generated messages use correct role conventions (user, operator, assistant, system)
5. DeepEval Synthesizer generates test scenarios and dataset examples as primary engine
6. OpenAI function calling orchestrates framework routing (not a fixed pipeline)
7. Hardcoded adapters convert framework outputs to Pydantic data contracts
8. Generated items include generator field in metadata tracking which framework produced them
9. Fallback to direct OpenAI generation if a framework call fails
</success_criteria>

<output>
After completion, create `.planning/phases/03-test-dataset-generation/03-04-SUMMARY.md`
</output>
