---
phase: 04-all-use-cases
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/dataset_generator/generation/format_adapters/__init__.py
  - src/dataset_generator/generation/format_adapters/base.py
  - src/dataset_generator/generation/format_adapters/single_turn_qa.py
  - src/dataset_generator/generation/format_adapters/operator_corrections.py
  - src/dataset_generator/generation/variation_router.py
  - src/dataset_generator/generation/source_classifier.py
  - src/dataset_generator/generation/fallback.py
autonomous: true

must_haves:
  truths:
    - "Support bot examples use single_turn_qa format with correct message structure"
    - "Operator quality examples use BOTH single_utterance_correction AND dialog_last_turn_correction formats"
    - "dialog_last_turn_correction examples have target_message_index pointing to last operator message"
    - "single_utterance_correction examples have exactly 1 message with role=operator"
    - "Test case parameters are generated using pairwise combinatorial testing (not full combinatorial)"
    - "Support bot metadata.source is classified as tickets, faq_paraphrase, or corner"
    - "Generated operator messages contain MIXED errors driven by parameter combinations (not one error type at a time)"
  artifacts:
    - path: "src/dataset_generator/generation/format_adapters/base.py"
      provides: "Abstract FormatAdapter base class"
      exports: ["FormatAdapter"]
    - path: "src/dataset_generator/generation/format_adapters/single_turn_qa.py"
      provides: "Support bot single-turn QA adapter"
      exports: ["SingleTurnQAAdapter"]
    - path: "src/dataset_generator/generation/format_adapters/operator_corrections.py"
      provides: "Both operator correction format adapters"
      exports: ["SingleUtteranceCorrectionAdapter", "DialogLastTurnCorrectionAdapter"]
    - path: "src/dataset_generator/generation/variation_router.py"
      provides: "Pairwise parameter variation using allpairspy"
      exports: ["generate_variations"]
    - path: "src/dataset_generator/generation/source_classifier.py"
      provides: "LLM-based metadata.source classification"
      exports: ["classify_source_type"]
  key_links:
    - from: "src/dataset_generator/generation/format_adapters/operator_corrections.py"
      to: "src/dataset_generator/models/dataset_example.py"
      via: "InputData with target_message_index"
      pattern: "target_message_index.*len.*messages.*-.*1"
    - from: "src/dataset_generator/generation/variation_router.py"
      to: "allpairspy"
      via: "AllPairs for pairwise combinatorial generation"
      pattern: "AllPairs"
    - from: "src/dataset_generator/generation/source_classifier.py"
      to: "OpenAI structured outputs"
      via: "LLM classification of metadata.source"
      pattern: "beta\\.chat\\.completions\\.parse"
---

<objective>
Build format-specific generation adapters, pairwise variation routing, and source type classification.

Purpose: The pipeline needs to generate dataset examples in 3 different formats (single_turn_qa, single_utterance_correction, dialog_last_turn_correction) with format-specific structural requirements. Each format needs dedicated generation logic that produces correct message structures, target_message_index values, and mixed error patterns. Test case parameter variations must use pairwise combinatorics to avoid exponential explosion. Support bot examples need metadata.source classification.

Output: Format adapter classes, variation router with allpairspy, source classifier, updated fallback generator
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-all-use-cases/04-CONTEXT.md
@.planning/phases/04-all-use-cases/04-RESEARCH.md
@.planning/phases/04-all-use-cases/04-01-SUMMARY.md
@tz.md
@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/models/test_case.py
@src/dataset_generator/generation/fallback.py
@src/dataset_generator/extraction/llm_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create format-specific generation adapters</name>
  <files>
    src/dataset_generator/generation/format_adapters/__init__.py
    src/dataset_generator/generation/format_adapters/base.py
    src/dataset_generator/generation/format_adapters/single_turn_qa.py
    src/dataset_generator/generation/format_adapters/operator_corrections.py
  </files>
  <action>
Create a format_adapters subdirectory under generation/ with format-specific adapters that use OpenAI to generate dataset examples in the correct structure.

**base.py — Abstract FormatAdapter:**
```python
class FormatAdapter(ABC):
    @abstractmethod
    def generate_example(self, use_case_id, test_case_id, parameters, policies, model, seed) -> DatasetExample
    @abstractmethod
    def validate_format(self, example: DatasetExample) -> list[str]  # Returns list of errors
    @abstractmethod
    def get_format_name(self) -> str
```

**single_turn_qa.py — SingleTurnQAAdapter:**
- Generates examples in single_turn_qa format (1 user message, expected_output is response)
- `generate_example`: Uses OpenAI structured outputs to generate a realistic user message and expected response IN RUSSIAN
  - System prompt describes: generate a customer support question and ideal response
  - Includes use case description and policies in prompt so output respects constraints
  - Parameters dict drives the scenario (e.g., tone=negative, has_order_id=false)
  - Sets `case` to the detected case value (passed via method or stored on adapter)
  - Sets `format` to "single_turn_qa"
  - Sets `metadata.source` to empty string (will be classified later by source_classifier)
- `validate_format`: Checks messages has exactly 1 message with role "user"
- Uses `get_openai_client()` for API calls, `temperature=0`

**operator_corrections.py — SingleUtteranceCorrectionAdapter AND DialogLastTurnCorrectionAdapter:**

`SingleUtteranceCorrectionAdapter`:
- Generates examples in single_utterance_correction format
- `generate_example`: Uses OpenAI to generate an operator message WITH ERRORS (driven by parameters) and its corrected version
  - System prompt: "Generate an operator message that contains errors matching the given parameters, and provide the corrected version"
  - Parameters drive WHICH errors appear: punctuation_errors, slang_profanity_emoji, caps_exclamation, medical_terms etc.
  - CRITICAL per user decision: errors must be MIXED — multiple error types in one message, driven by parameter combinations. NOT one error at a time.
  - The prompt must instruct the LLM to include ALL non-default parameter values as simultaneous errors
  - Input: 1 message with role="operator", content=the erroneous message
  - expected_output: the corrected version
  - target_message_index: 0 (point to the single operator message)
  - Sets case="operator_quality", format="single_utterance_correction"
- `validate_format`: Checks messages has exactly 1 message, role is "operator", target_message_index is 0

`DialogLastTurnCorrectionAdapter`:
- Generates examples in dialog_last_turn_correction format
- `generate_example`: Uses OpenAI to generate a multi-turn dialog (min 2 messages) ending with an operator message that has errors, plus the corrected version
  - System prompt: "Generate a dialog between user and operator. The last message must be from the operator and should contain errors matching parameters. Provide the corrected last reply."
  - Dialog structure: user message(s) -> ... -> operator message (with errors)
  - Parameters drive the same error axes as SingleUtteranceCorrectionAdapter
  - Per tz.md: minimum 2 messages (user then operator)
  - Per tz.md and user decision: dialog can be longer (3-5 messages typical)
  - Escalation responses include full text as shown in tz.md canonical examples
  - target_message_index = len(messages) - 1
  - messages[-1].role MUST be "operator"
  - Sets case="operator_quality", format="dialog_last_turn_correction"
- `validate_format`: Checks messages has 2+ messages, last message role is "operator", target_message_index == len(messages)-1

**__init__.py:**
Export all adapter classes and provide a factory function:
```python
def get_adapter_for_format(format_name: str, case: str) -> FormatAdapter:
    """Return the appropriate adapter for a format."""
```
Mapping: single_turn_qa -> SingleTurnQAAdapter, single_utterance_correction -> SingleUtteranceCorrectionAdapter, dialog_last_turn_correction -> DialogLastTurnCorrectionAdapter.

ALL generation prompts must be GENERIC per user locked decision — no case-specific few-shot examples. The use case description and policies guide generation, not hardcoded templates.

ALL generated text must be in RUSSIAN (matching input documents).
  </action>
  <verify>
Run: `python3 -c "
from dataset_generator.generation.format_adapters import get_adapter_for_format, SingleTurnQAAdapter, SingleUtteranceCorrectionAdapter, DialogLastTurnCorrectionAdapter
from dataset_generator.generation.format_adapters.base import FormatAdapter
# Verify all adapters are importable
qa = get_adapter_for_format('single_turn_qa', 'support_bot')
assert isinstance(qa, SingleTurnQAAdapter)
suc = get_adapter_for_format('single_utterance_correction', 'operator_quality')
assert isinstance(suc, SingleUtteranceCorrectionAdapter)
dlc = get_adapter_for_format('dialog_last_turn_correction', 'operator_quality')
assert isinstance(dlc, DialogLastTurnCorrectionAdapter)
print('All format adapters verified')
"` — should print 'All format adapters verified'
  </verify>
  <done>3 format adapters created (SingleTurnQAAdapter, SingleUtteranceCorrectionAdapter, DialogLastTurnCorrectionAdapter) with format-specific generation and validation, factory function available</done>
</task>

<task type="auto">
  <name>Task 2: Build variation router, source classifier, and update fallback generator</name>
  <files>
    src/dataset_generator/generation/variation_router.py
    src/dataset_generator/generation/source_classifier.py
    src/dataset_generator/generation/fallback.py
  </files>
  <action>
**variation_router.py — Pairwise parameter variation:**
Create a module with function `generate_variations(case: str, use_case_description: str, policies: list, min_test_cases: int = 3) -> list[dict]`:

1. Define default variation axes per case (per tz.md):
   - support_bot: tone (neutral/negative/aggressive), has_order_id (true/false), requires_account_access (true/false), language (ru/en), adversarial (none/profanity/injection/garbage)
   - operator_quality: phrase_length (short/medium/long), punctuation_errors (none/minor/severe), slang_profanity_emoji (none/moderate/excessive), medical_terms (none/present), user_aggression (neutral/frustrated/angry), escalation_needed (no/yes)
   - doctor_booking: reuse support_bot axes (similar domain)

2. Use `allpairspy.AllPairs` to generate pairwise combinations
3. If pairwise produces fewer than min_test_cases combinations, pad with additional random combinations
4. Return list of parameter dicts, each dict maps axis_name -> value

5. For each returned parameter dict, also select 2-3 axes names to use as `parameter_variation_axes` on TestCase (pick the axes that have non-default/non-neutral values in that combination)

**source_classifier.py — Metadata source classification:**
Create a module with function `classify_source_type(use_case_description: str, generated_input: str, parameters: dict, model: str = "gpt-4o-mini") -> str`:

1. Quick heuristic checks first (avoid LLM call when obvious):
   - If parameters contain adversarial=True or adversarial in (profanity, injection, garbage) -> return "corner"
   - If "FAQ" or "faq" appears in use_case_description and parameters don't indicate adversarial -> return "faq_paraphrase"

2. For non-obvious cases, use OpenAI structured outputs with a Pydantic model:
   - `SourceClassification(source: Literal["tickets", "faq_paraphrase", "corner"], confidence: float)`
   - System prompt: classify whether this example is from ticket data, FAQ paraphrase, or a corner/adversarial case
   - temperature=0

3. Return the source string

4. This classifier is ONLY used for support_bot case. For other cases, metadata.source is not set (or empty).

**fallback.py — Update to be format-aware:**
Update the existing fallback generator to be format-aware:

1. Add `case` and `formats` parameters to `generate_with_openai_fallback`:
   `generate_with_openai_fallback(use_case_id, use_case_description, policies, num_test_cases=3, model="gpt-4o-mini", seed=None, case="support_bot", formats=None)`

2. Update `_build_system_prompt` to include:
   - The `case` value so generated examples use the correct case field
   - The `formats` list so the LLM generates examples in the correct format(s)
   - For operator_quality: instruct to generate single_utterance_correction (1 operator message) AND dialog_last_turn_correction (multi-turn dialog ending with operator)
   - For dialog_last_turn_correction: include target_message_index in the JSON output format example
   - For single_utterance_correction: include target_message_index: 0

3. Update the DatasetExample construction to:
   - Set `case` from the parameter (not hardcoded "support_bot")
   - Set `format` from the LLM response or from the formats parameter
   - Set `input.target_message_index` when format is single_utterance_correction or dialog_last_turn_correction

4. Keep backward compatibility: if case/formats not provided, default to support_bot/single_turn_qa behavior.
  </action>
  <verify>
Run: `python3 -c "
from dataset_generator.generation.variation_router import generate_variations
from dataset_generator.generation.source_classifier import classify_source_type
# Test variation generation
vars_support = generate_variations('support_bot', 'Handle FAQ queries', [], min_test_cases=3)
assert len(vars_support) >= 3
assert all(isinstance(v, dict) for v in vars_support)
vars_operator = generate_variations('operator_quality', 'Check operator messages', [], min_test_cases=3)
assert len(vars_operator) >= 3
# Test source classifier heuristics
source = classify_source_type('FAQ support', 'test', {'adversarial': 'profanity'})
assert source == 'corner'
source2 = classify_source_type('FAQ about delivery', 'test', {})
assert source2 == 'faq_paraphrase'
print('Variation router and source classifier verified')
"` — should print 'Variation router and source classifier verified'

Run: `python3 -c "
from dataset_generator.generation.fallback import generate_with_openai_fallback
import inspect
sig = inspect.signature(generate_with_openai_fallback)
assert 'case' in sig.parameters
assert 'formats' in sig.parameters
print('Fallback generator updated')
"` — should print 'Fallback generator updated'
  </verify>
  <done>Variation router generates pairwise parameter combinations using allpairspy, source classifier provides metadata.source for support_bot, fallback generator is format-aware with case/formats parameters</done>
</task>

</tasks>

<verification>
1. Format adapters generate examples in correct structure per tz.md canonical examples
2. SingleUtteranceCorrectionAdapter: 1 operator message, target_message_index=0
3. DialogLastTurnCorrectionAdapter: 2+ messages, last is operator, target_message_index=len-1
4. SingleTurnQAAdapter: 1 user message, no target_message_index
5. Variation router produces pairwise combinations, not full combinatorial
6. Source classifier returns tickets/faq_paraphrase/corner for support_bot case
7. Fallback generator accepts case/formats parameters
</verification>

<success_criteria>
- All 3 format adapters importable and produce structurally correct examples
- Variation router generates 3+ parameter combinations per case using allpairspy pairwise
- Source classifier heuristics work for obvious cases (corner, faq_paraphrase)
- Fallback generator supports all 3 formats including target_message_index
</success_criteria>

<output>
After completion, create `.planning/phases/04-all-use-cases/04-02-SUMMARY.md`
</output>
