---
phase: 04-all-use-cases
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/dataset_generator/models/dataset_example.py
  - src/dataset_generator/models/use_case.py
  - src/dataset_generator/models/policy.py
  - src/dataset_generator/models/test_case.py
  - src/dataset_generator/models/__init__.py
  - src/dataset_generator/generation/case_detector.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Pipeline can auto-detect case field (support_bot, operator_quality, doctor_booking) from document content without filename"
    - "Pipeline can auto-detect applicable formats (single_turn_qa, single_utterance_correction, dialog_last_turn_correction) from document content"
    - "One document can produce examples in MULTIPLE formats"
    - "InputData supports target_message_index for dialog_last_turn_correction format"
    - "UseCase, Policy, TestCase models include case field matching tz.md contract"
    - "TestCase model includes parameters dict and policy_ids list matching tz.md contract"
  artifacts:
    - path: "src/dataset_generator/generation/case_detector.py"
      provides: "LLM-based case and format auto-detection"
      exports: ["detect_case_and_formats"]
    - path: "src/dataset_generator/models/dataset_example.py"
      provides: "InputData with optional target_message_index"
      contains: "target_message_index"
    - path: "src/dataset_generator/models/test_case.py"
      provides: "TestCase with case, parameters, policy_ids fields"
      contains: "parameters"
    - path: "src/dataset_generator/models/use_case.py"
      provides: "UseCase with case field"
      contains: "case"
    - path: "src/dataset_generator/models/policy.py"
      provides: "Policy with case and statement fields"
      contains: "statement"
  key_links:
    - from: "src/dataset_generator/generation/case_detector.py"
      to: "OpenAI structured outputs API"
      via: "client.beta.chat.completions.parse with CaseFormatDetection Pydantic model"
      pattern: "beta\\.chat\\.completions\\.parse"
    - from: "src/dataset_generator/models/dataset_example.py"
      to: "tz.md data contract"
      via: "target_message_index field on InputData"
      pattern: "target_message_index.*Optional\\[int\\]"
---

<objective>
Update data models to match tz.md data contract and build LLM-based case/format auto-detection.

Purpose: The existing models are missing fields required by tz.md (case on UseCase/Policy/TestCase, target_message_index on InputData, parameters/policy_ids on TestCase, statement on Policy). The pipeline also needs universal case/format detection that works on any document without relying on filenames. This plan closes both gaps.

Output: Updated Pydantic models matching tz.md contract + case_detector.py module + allpairspy dependency
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-all-use-cases/04-CONTEXT.md
@.planning/phases/04-all-use-cases/04-RESEARCH.md
@.planning/phases/03-test-dataset-generation/03-01-SUMMARY.md
@tz.md
@src/dataset_generator/models/dataset_example.py
@src/dataset_generator/models/use_case.py
@src/dataset_generator/models/policy.py
@src/dataset_generator/models/test_case.py
@src/dataset_generator/models/__init__.py
@src/dataset_generator/extraction/use_case_extractor.py
@src/dataset_generator/extraction/policy_extractor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update data models to match tz.md data contract</name>
  <files>
    src/dataset_generator/models/dataset_example.py
    src/dataset_generator/models/use_case.py
    src/dataset_generator/models/policy.py
    src/dataset_generator/models/test_case.py
    src/dataset_generator/models/__init__.py
  </files>
  <action>
Update existing Pydantic models to include ALL fields required by tz.md data contract. These are ADDITIVE changes — do NOT remove or rename existing fields. Add new fields with defaults so existing code continues to work.

**dataset_example.py — InputData:**
- Add `target_message_index: Optional[int] = Field(default=None, description="For dialog_last_turn_correction: index of operator message to correct")`
- Add a field_validator on target_message_index that:
  1. If value is not None, checks it's within range of messages list
  2. If value is not None, checks that messages[target_message_index].role == "operator"
  Note: Use model_validator(mode='after') for cross-field validation since field_validator can't access other fields reliably in v2.

**use_case.py — UseCase:**
- Add `case: str = Field(default="", description="Case identifier (e.g., support_bot, operator_quality, doctor_booking)")`
- This field is optional (default empty string) because Phase 2 extractors don't set it yet — Phase 4 pipeline will populate it after extraction via case detection.

**policy.py — Policy:**
- Add `case: str = Field(default="", description="Case identifier")`
- Add `statement: str = Field(default="", description="Policy statement per tz.md contract (alias for description)")`
- Note: tz.md uses `statement` but our existing code uses `description`. Keep both. In serialization, populate `statement` from `description` if empty. Add a model_validator(mode='after') that sets statement=description if statement is empty.

**test_case.py — TestCase:**
- Add `case: str = Field(default="", description="Case identifier")`
- Add `parameters: dict = Field(default_factory=dict, description="Test case parameter values per tz.md (e.g., tone=negative, has_order_id=true)")`
- Add `policy_ids: list[str] = Field(default_factory=list, description="List of relevant policy IDs")`
- Add a field_validator on policy_ids that checks each starts with "pol_" (only if list is non-empty)

**__init__.py:**
- No changes needed if all new exports are already through existing classes. Just verify imports still work.

IMPORTANT: All new fields have defaults so existing Phase 1-3 code that constructs these models without the new fields will NOT break.
  </action>
  <verify>
Run: `python3 -c "
from dataset_generator.models import UseCase, Policy, TestCase, DatasetExample, InputData, Message
# Verify new fields exist with defaults
uc = UseCase(id='uc_test', name='Test', description='Test', evidence=[], case='support_bot')
assert uc.case == 'support_bot'
pol = Policy(id='pol_test', name='Test', description='Test desc', type='must', evidence=[])
assert pol.statement == 'Test desc'  # auto-populated from description
tc = TestCase(id='tc_test', use_case_id='uc_test', name='Test', description='D', parameter_variation_axes=['a','b'], parameters={'tone': 'negative'}, policy_ids=['pol_001'], case='support_bot')
assert tc.parameters == {'tone': 'negative'}
assert tc.policy_ids == ['pol_001']
# Verify target_message_index
inp = InputData(messages=[Message(role='operator', content='Hi')], target_message_index=0)
assert inp.target_message_index == 0
# Verify backward compatibility (no new fields required)
uc2 = UseCase(id='uc_old', name='Old', description='Old', evidence=[])
assert uc2.case == ''
print('All model updates verified')
"` — should print 'All model updates verified'
  </verify>
  <done>All 4 model files updated with tz.md-required fields, backward compatible with existing code, target_message_index validated on InputData</done>
</task>

<task type="auto">
  <name>Task 2: Build case/format auto-detection module and add allpairspy dependency</name>
  <files>
    src/dataset_generator/generation/case_detector.py
    pyproject.toml
  </files>
  <action>
**pyproject.toml:**
- Add `allpairspy` to dependencies list (no version pin needed, latest is fine)

**case_detector.py:**
Create a new module with a single public function `detect_case_and_formats` that uses OpenAI structured outputs to classify a document's case and applicable formats.

Implementation:
1. Define a Pydantic model `CaseFormatDetection`:
   - `case: Literal["support_bot", "operator_quality", "doctor_booking"]`
   - `formats: list[Literal["single_turn_qa", "single_utterance_correction", "dialog_last_turn_correction"]]` — can have MULTIPLE formats
   - `reasoning: str` — brief explanation of classification

2. Define function `detect_case_and_formats(use_cases: list, policies: list, model: str = "gpt-4o-mini") -> CaseFormatDetection`:
   - Takes extracted use cases and policies (from Phase 2 extractors)
   - Builds a summary string from use case names/descriptions and policy names/types/descriptions
   - Calls `client.beta.chat.completions.parse()` with `response_format=CaseFormatDetection` and `temperature=0`
   - System prompt instructs the LLM to:
     - Classify the domain as support_bot (FAQ/tickets/customer support), operator_quality (message correction/quality checks), or doctor_booking (medical appointment booking)
     - Detect ALL applicable dataset formats — support_bot uses single_turn_qa; operator_quality uses BOTH single_utterance_correction AND dialog_last_turn_correction; doctor_booking uses single_turn_qa
     - NEVER reference filenames — classify based on content ONLY
   - Uses `get_openai_client()` from `extraction.llm_client`
   - Returns the parsed CaseFormatDetection object

3. CRITICAL per user locked decision: The detection function receives ONLY content (use case descriptions, policy descriptions). It must NOT receive or use the input filename. This ensures universality — same document renamed produces same classification.

4. Handle edge cases:
   - If LLM returns empty formats list, default to ["single_turn_qa"]
   - If parsing fails, log warning and return a default CaseFormatDetection with case="support_bot", formats=["single_turn_qa"]
  </action>
  <verify>
Run: `python3 -c "
from dataset_generator.generation.case_detector import detect_case_and_formats, CaseFormatDetection
# Verify module is importable and CaseFormatDetection model works
det = CaseFormatDetection(case='operator_quality', formats=['single_utterance_correction', 'dialog_last_turn_correction'], reasoning='test')
assert det.case == 'operator_quality'
assert len(det.formats) == 2
print('case_detector module verified')
"` — should print 'case_detector module verified'

Run: `pip3 install allpairspy && python3 -c "from allpairspy import AllPairs; print('allpairspy installed')"` — should print 'allpairspy installed'
  </verify>
  <done>case_detector.py module created with detect_case_and_formats function using OpenAI structured outputs, allpairspy dependency added to pyproject.toml and installable</done>
</task>

</tasks>

<verification>
1. All 4 model files have new fields matching tz.md data contract
2. InputData.target_message_index validates role=operator and index range
3. Policy.statement auto-populated from description
4. TestCase has case, parameters, policy_ids fields
5. case_detector.py importable with CaseFormatDetection model
6. allpairspy importable
7. Backward compatibility: existing model construction without new fields still works
</verification>

<success_criteria>
- All Pydantic models pass validation with new fields
- Existing code (pipeline.py, orchestrator.py, fallback.py) still works without changes
- case_detector.py provides detect_case_and_formats that returns case + list of formats
- allpairspy available for Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/04-all-use-cases/04-01-SUMMARY.md`
</output>
