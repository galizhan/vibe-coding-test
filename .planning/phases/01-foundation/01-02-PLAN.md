---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/dataset_generator/extraction/__init__.py
  - src/dataset_generator/extraction/markdown_parser.py
  - src/dataset_generator/extraction/use_case_extractor.py
  - src/dataset_generator/extraction/policy_extractor.py
  - src/dataset_generator/extraction/evidence_validator.py
  - src/dataset_generator/extraction/llm_client.py
autonomous: true

must_haves:
  truths:
    - "Markdown parser reads a file and returns full text plus a list of lines with 1-based indexing"
    - "Use case extractor calls OpenAI API and returns a list of UseCase objects with evidence"
    - "Policy extractor calls OpenAI API and returns a list of Policy objects with evidence"
    - "Evidence validator checks that each evidence.quote exactly matches source lines at line_start:line_end"
    - "LLM calls use temperature=0 and pass seed when provided"
    - "All LLM calls have retry logic for rate limit errors"
  artifacts:
    - path: "src/dataset_generator/extraction/markdown_parser.py"
      provides: "Line-tracking markdown parser"
      contains: "parse_markdown_with_lines"
    - path: "src/dataset_generator/extraction/llm_client.py"
      provides: "OpenAI client wrapper with retry logic"
      contains: "call_openai_structured"
    - path: "src/dataset_generator/extraction/use_case_extractor.py"
      provides: "Use case extraction from markdown via LLM"
      contains: "extract_use_cases"
    - path: "src/dataset_generator/extraction/policy_extractor.py"
      provides: "Policy extraction from markdown via LLM"
      contains: "extract_policies"
    - path: "src/dataset_generator/extraction/evidence_validator.py"
      provides: "Evidence quote validation against source text"
      contains: "validate_evidence"
  key_links:
    - from: "src/dataset_generator/extraction/use_case_extractor.py"
      to: "src/dataset_generator/extraction/llm_client.py"
      via: "calls call_openai_structured with UseCaseList response_format"
      pattern: "call_openai_structured"
    - from: "src/dataset_generator/extraction/policy_extractor.py"
      to: "src/dataset_generator/extraction/llm_client.py"
      via: "calls call_openai_structured with PolicyList response_format"
      pattern: "call_openai_structured"
    - from: "src/dataset_generator/extraction/use_case_extractor.py"
      to: "src/dataset_generator/models/use_case.py"
      via: "imports UseCaseList as response_format for structured output"
      pattern: "from.*models.*import.*UseCaseList"
    - from: "src/dataset_generator/extraction/evidence_validator.py"
      to: "src/dataset_generator/extraction/markdown_parser.py"
      via: "uses parsed lines to validate evidence quotes"
      pattern: "ParsedMarkdown"
---

<objective>
Implement the core extraction pipeline: markdown parser with line tracking, OpenAI-powered use case and policy extractors with structured outputs, and evidence validation that checks quotes against source text.

Purpose: This is the core intelligence of the system -- turning unstructured markdown into structured, evidence-backed use cases and policies. Without this, the CLI is an empty shell.
Output: Working extraction functions that can be called from CLI in Plan 03.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Markdown parser and LLM client with retry logic</name>
  <files>
    src/dataset_generator/extraction/markdown_parser.py
    src/dataset_generator/extraction/llm_client.py
    src/dataset_generator/extraction/evidence_validator.py
    src/dataset_generator/extraction/__init__.py
  </files>
  <action>
    Create src/dataset_generator/extraction/markdown_parser.py:
    - NamedTuple or dataclass ParsedMarkdown with fields: full_text (str), lines (list[str]), file_path (Path)
    - Function parse_markdown_with_lines(file_path: Path) -> ParsedMarkdown
      - Reads file with encoding='utf-8'
      - Strips \r\n and \r line endings (normalize to \n)
      - Returns ParsedMarkdown with lines as a list (0-indexed internally, evidence uses 1-based)
    - Function get_numbered_text(parsed: ParsedMarkdown) -> str
      - Returns the full text with line numbers prepended for LLM context: "1: line content\n2: line content\n..."
      - This helps the LLM see line numbers when extracting evidence

    Create src/dataset_generator/extraction/llm_client.py:
    - Import OpenAI, tenacity, RateLimitError
    - Function get_openai_client() -> OpenAI (reads from env, no hardcoded keys)
    - Function call_openai_structured(
        messages: list[dict],
        response_format: type[BaseModel],
        model: str = "gpt-4o-mini",
        seed: int | None = None,
        temperature: float = 0,
      ) -> BaseModel
      - Wraps client.beta.chat.completions.parse()
      - Decorated with @retry from tenacity: wait_random_exponential(min=1, max=60), stop_after_attempt(6), retry_if_exception_type(RateLimitError)
      - Passes temperature=0 always (per REPR-02 requirement)
      - Passes seed if not None
      - Returns response.choices[0].message.parsed

    Create src/dataset_generator/extraction/evidence_validator.py:
    - Function validate_evidence_quote(parsed: ParsedMarkdown, evidence: Evidence) -> tuple[bool, str]
      - Converts 1-based line_start/line_end to 0-based slice
      - Extracts actual text from parsed.lines[start_idx:end_idx]
      - Joins with \n
      - Compares with evidence.quote (normalize both sides: strip trailing whitespace per line, normalize \r\n)
      - Returns (True, "") or (False, "mismatch description")
    - Function validate_all_evidence(parsed: ParsedMarkdown, items: list) -> tuple[int, int, list[str]]
      - items can be UseCases or Policies (both have .evidence list)
      - Returns (valid_count, invalid_count, list_of_error_messages)

    Update src/dataset_generator/extraction/__init__.py to export key functions.

    IMPORTANT: The evidence validator must handle edge cases:
    - Trailing whitespace differences (strip each line before comparison)
    - Empty lines at boundaries
    - Out-of-bounds line numbers (return False with descriptive error)
  </action>
  <verify>
    Run: `python -c "
from pathlib import Path
from dataset_generator.extraction.markdown_parser import parse_markdown_with_lines, get_numbered_text
# Parse one of the example files
parsed = parse_markdown_with_lines(Path('example_input_raw_support_faq_and_tickets.md'))
print(f'Lines: {len(parsed.lines)}')
print(f'First line: {parsed.lines[0]!r}')
numbered = get_numbered_text(parsed)
print(f'Numbered first line: {numbered.split(chr(10))[0]!r}')
print('Parser OK')
"`
    Run: `python -c "
from dataset_generator.extraction.llm_client import get_openai_client
# Just verify the client can be constructed (don't call API)
import os
os.environ.setdefault('OPENAI_API_KEY', 'test-key')
client = get_openai_client()
print(f'Client type: {type(client).__name__}')
print('LLM client OK')
"`
    Run: `python -c "
from dataset_generator.extraction.markdown_parser import parse_markdown_with_lines, ParsedMarkdown
from dataset_generator.extraction.evidence_validator import validate_evidence_quote
from dataset_generator.models import Evidence
from pathlib import Path
parsed = parse_markdown_with_lines(Path('example_input_raw_support_faq_and_tickets.md'))
# Create evidence matching the first line
ev = Evidence(input_file='test.md', line_start=1, line_end=1, quote=parsed.lines[0])
valid, msg = validate_evidence_quote(parsed, ev)
print(f'Valid: {valid}, msg: {msg!r}')
# Create bad evidence
ev_bad = Evidence(input_file='test.md', line_start=1, line_end=1, quote='completely wrong text')
valid2, msg2 = validate_evidence_quote(parsed, ev_bad)
print(f'Valid: {valid2}, msg: {msg2[:50]!r}...')
print('Validator OK')
"`
  </verify>
  <done>
    Markdown parser reads files with line tracking. LLM client wraps OpenAI with retry logic and structured output. Evidence validator correctly identifies matching and mismatching quotes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Use case and policy extractors with LLM structured outputs</name>
  <files>
    src/dataset_generator/extraction/use_case_extractor.py
    src/dataset_generator/extraction/policy_extractor.py
  </files>
  <action>
    Create src/dataset_generator/extraction/use_case_extractor.py:
    - Function extract_use_cases(
        parsed: ParsedMarkdown,
        model: str = "gpt-4o-mini",
        seed: int | None = None,
        min_use_cases: int = 5,
      ) -> UseCaseList
    - Build system prompt that instructs the LLM to:
      - Extract structured use cases from the provided Russian-language requirements document
      - Each use case must have a unique id starting with "uc_" (e.g., uc_001, uc_002)
      - Each use case needs: id, name (Russian), description (Russian), evidence
      - Evidence must have: input_file (filename from context), line_start (1-based), line_end (1-based), quote (exact text from those lines)
      - Extract at least {min_use_cases} use cases
      - ALL content (name, description) MUST be in Russian (per GEN-05)
      - Quotes must be EXACT text from the source -- not paraphrased, not translated
    - User message: prepend line numbers to the markdown text using get_numbered_text(parsed)
      - Include the filename: f"File: {parsed.file_path.name}\n\n{numbered_text}"
    - Call call_openai_structured with response_format=UseCaseList
    - After extraction, run validate_all_evidence to check quotes
    - Log warning if any evidence is invalid but still return results (validation is informational in Phase 1, will be strict later)

    Create src/dataset_generator/extraction/policy_extractor.py:
    - Function extract_policies(
        parsed: ParsedMarkdown,
        model: str = "gpt-4o-mini",
        seed: int | None = None,
        min_policies: int = 5,
      ) -> PolicyList
    - Build system prompt that instructs the LLM to:
      - Extract policies/rules/constraints from the provided Russian-language requirements document
      - Each policy must have a unique id starting with "pol_" (e.g., pol_001, pol_002)
      - Each policy needs: id, name (Russian), description (Russian), type (one of: must, must_not, escalate, style, format), evidence
      - Classify policies by type:
        - "must": things the system must do
        - "must_not": things the system must not do
        - "escalate": situations requiring escalation to a human
        - "style": tone, language, communication style rules
        - "format": output format requirements
      - Evidence must have: input_file, line_start, line_end, quote (exact text)
      - Extract at least {min_policies} policies
      - ALL content (name, description) MUST be in Russian (per GEN-05)
      - Quotes must be EXACT text from the source
    - User message: same pattern as use_case_extractor (numbered text + filename)
    - Call call_openai_structured with response_format=PolicyList
    - After extraction, run validate_all_evidence to check quotes

    CRITICAL PROMPT ENGINEERING NOTES:
    - Include line-numbered text so the LLM can reference exact line numbers
    - Explicitly instruct: "The quote field must contain the EXACT text from the specified lines. Copy it character-for-character. Do NOT paraphrase or modify."
    - Explicitly instruct: "Line numbers are shown at the start of each line as 'N: '. Use these to set line_start and line_end. Do NOT include the line number prefix in the quote."
    - The input_file field should be set to the filename (e.g., "example_input_raw_support_faq_and_tickets.md")
  </action>
  <verify>
    This task requires an actual OpenAI API key to fully verify. Run a quick structural check:
    Run: `python -c "
from dataset_generator.extraction.use_case_extractor import extract_use_cases
from dataset_generator.extraction.policy_extractor import extract_policies
print('Extractors importable OK')
# Check function signatures
import inspect
uc_sig = inspect.signature(extract_use_cases)
pol_sig = inspect.signature(extract_policies)
print(f'extract_use_cases params: {list(uc_sig.parameters.keys())}')
print(f'extract_policies params: {list(pol_sig.parameters.keys())}')
"`
    If OPENAI_API_KEY is available, run a live test:
    `python -c "
from pathlib import Path
from dataset_generator.extraction.markdown_parser import parse_markdown_with_lines
from dataset_generator.extraction.use_case_extractor import extract_use_cases
parsed = parse_markdown_with_lines(Path('example_input_raw_support_faq_and_tickets.md'))
result = extract_use_cases(parsed, seed=42)
print(f'Extracted {len(result.use_cases)} use cases')
for uc in result.use_cases:
    print(f'  {uc.id}: {uc.name}')
    for ev in uc.evidence:
        print(f'    Evidence: lines {ev.line_start}-{ev.line_end}')
"`
  </verify>
  <done>
    Use case extractor calls OpenAI structured outputs API and returns UseCaseList with evidence. Policy extractor does the same for PolicyList. Both use temperature=0, pass seed, include Russian language instruction, and provide line-numbered source text for accurate evidence extraction. Evidence validation runs after extraction with warnings logged.
  </done>
</task>

</tasks>

<verification>
1. Markdown parser correctly reads example files with line tracking
2. LLM client has retry logic and structured output support
3. Evidence validator correctly validates/rejects quotes against source lines
4. Extractors are importable with correct function signatures
5. (With API key) Extractors produce UseCaseList and PolicyList from example markdown
</verification>

<success_criteria>
- parse_markdown_with_lines reads files and returns numbered lines
- call_openai_structured wraps OpenAI API with retry and structured output
- extract_use_cases returns UseCaseList with uc_-prefixed IDs and evidence
- extract_policies returns PolicyList with pol_-prefixed IDs, typed policies, and evidence
- validate_evidence_quote correctly validates exact quote matching
- All prompts instruct Russian language output (GEN-05) and exact quote extraction
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
