---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - src/dataset_generator/cli.py
  - src/dataset_generator/pipeline.py
  - src/dataset_generator/utils/__init__.py
  - src/dataset_generator/utils/file_writer.py
autonomous: false

must_haves:
  truths:
    - "User runs python -m dataset_generator generate --input <file> --out <dir> and gets use_cases.json + policies.json"
    - "Evidence fields (line_start, line_end, quote) match actual source document text"
    - "All output passes Pydantic validation with correct ID prefixes (uc_, pol_) and mandatory fields"
    - "Running with --seed produces structurally consistent extraction results"
    - "CLI reads OPENAI_API_KEY from env and supports --model switching"
    - "Output JSON files have ensure_ascii=False for Russian text and indent=2 for readability"
  artifacts:
    - path: "src/dataset_generator/pipeline.py"
      provides: "Orchestrates full extraction pipeline: parse -> extract UCs -> extract policies -> validate -> write"
      contains: "run_pipeline"
    - path: "src/dataset_generator/utils/file_writer.py"
      provides: "JSON file output with Pydantic serialization"
      contains: "write_json_output"
    - path: "src/dataset_generator/cli.py"
      provides: "Fully wired CLI generate command"
      contains: "generate"
  key_links:
    - from: "src/dataset_generator/cli.py"
      to: "src/dataset_generator/pipeline.py"
      via: "generate command calls run_pipeline"
      pattern: "run_pipeline"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/extraction/markdown_parser.py"
      via: "parses input file"
      pattern: "parse_markdown_with_lines"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/extraction/use_case_extractor.py"
      via: "extracts use cases from parsed markdown"
      pattern: "extract_use_cases"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/extraction/policy_extractor.py"
      via: "extracts policies from parsed markdown"
      pattern: "extract_policies"
    - from: "src/dataset_generator/pipeline.py"
      to: "src/dataset_generator/utils/file_writer.py"
      via: "writes JSON output files"
      pattern: "write_json_output"
---

<objective>
Wire everything together: create the pipeline orchestrator that connects parsing, extraction, and output writing, update the CLI to call it, and verify end-to-end with a real markdown file.

Purpose: This plan delivers the Phase 1 promise -- the user can actually run the CLI and get validated JSON output from a markdown document.
Output: Working end-to-end CLI that produces use_cases.json and policies.json from markdown input.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline orchestrator and JSON file writer</name>
  <files>
    src/dataset_generator/utils/__init__.py
    src/dataset_generator/utils/file_writer.py
    src/dataset_generator/pipeline.py
    src/dataset_generator/cli.py
  </files>
  <action>
    Create src/dataset_generator/utils/__init__.py (empty).

    Create src/dataset_generator/utils/file_writer.py:
    - Function write_json_output(model: BaseModel, output_path: Path) -> None
      - Creates parent directories with mkdir(parents=True, exist_ok=True)
      - Writes model_dump_json(indent=2) to file with encoding='utf-8'
      - CRITICAL: For Russian text, the JSON serialization must use ensure_ascii=False.
        Pydantic v2 model_dump_json does NOT have ensure_ascii parameter.
        Instead: use json.dumps(model.model_dump(), indent=2, ensure_ascii=False) to write readable Russian.
      - Prints confirmation message to stderr (not stdout): f"Wrote {output_path}"

    Create src/dataset_generator/pipeline.py:
    - Import parse_markdown_with_lines, extract_use_cases, extract_policies, validate_all_evidence, write_json_output
    - dataclass PipelineConfig with: input_file (Path), out_dir (Path), seed (int | None), model (str), n_use_cases (int), n_test_cases_per_uc (int), n_examples_per_tc (int)
    - dataclass PipelineResult with: use_cases_path (Path), policies_path (Path), use_case_count (int), policy_count (int), evidence_valid (int), evidence_invalid (int)
    - Function run_pipeline(config: PipelineConfig) -> PipelineResult:
      1. Parse markdown: parsed = parse_markdown_with_lines(config.input_file)
      2. Extract use cases: uc_list = extract_use_cases(parsed, model=config.model, seed=config.seed, min_use_cases=config.n_use_cases)
      3. Extract policies: pol_list = extract_policies(parsed, model=config.model, seed=config.seed)
      4. Validate evidence for both use cases and policies
      5. Write use_cases.json to config.out_dir / "use_cases.json"
      6. Write policies.json to config.out_dir / "policies.json"
      7. Print summary: counts, evidence validation results
      8. Return PipelineResult with paths and counts

    Update src/dataset_generator/cli.py:
    - Import PipelineConfig, run_pipeline from .pipeline
    - Replace placeholder body in generate command with:
      1. Check OPENAI_API_KEY env var (exit code 1 if missing)
      2. Create PipelineConfig from CLI arguments
      3. Call run_pipeline(config)
      4. Print summary: "Generated {N} use cases, {M} policies"
      5. Print evidence validation summary
      6. Print output paths
    - Use typer.echo for all output
    - Wrap in try/except to catch OpenAI errors and print user-friendly messages

    IMPORTANT: The generate command must handle the case where OPENAI_API_KEY is not set BEFORE attempting any API calls. Use os.getenv("OPENAI_API_KEY") check.
  </action>
  <verify>
    Run: `python -c "from dataset_generator.pipeline import run_pipeline, PipelineConfig; print('Pipeline importable')"`.
    Run: `python -c "from dataset_generator.utils.file_writer import write_json_output; print('Writer importable')"`.
    Run: `python -m dataset_generator generate --help` still shows all parameters.
    Run (without API key): `unset OPENAI_API_KEY && python -m dataset_generator generate --input example_input_raw_support_faq_and_tickets.md --out /tmp/test_out` should exit with error about missing API key.
  </verify>
  <done>
    Pipeline orchestrator connects parsing, extraction, validation, and output writing. CLI generate command is fully wired. Missing API key produces a clear error message. Writer handles Russian text with ensure_ascii=False.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end verification with real markdown file</name>
  <files>no files modified -- verification only</files>
  <action>
    This is a human verification checkpoint. Claude should run the pipeline end-to-end before presenting results to the user:

    1. Run the pipeline with the support FAQ example:
       python -m dataset_generator generate --input example_input_raw_support_faq_and_tickets.md --out /tmp/phase1_test --seed 42 --model gpt-4o-mini

    2. Run the pipeline with the operator quality example:
       python -m dataset_generator generate --input example_input_raw_operator_quality_checks.md --out /tmp/phase1_test_operator --seed 42

    3. Validate outputs exist and contain expected structure.

    4. Present results to the user for visual verification of:
       - Russian content quality
       - Evidence accuracy (do quotes match source lines?)
       - Reasonable use case and policy extraction
       - Seed reproducibility (run twice and compare structure)
  </action>
  <verify>
    User confirms:
    - use_cases.json has 5+ use cases with uc_ IDs and Russian content
    - policies.json has 5+ policies with pol_ IDs, valid types, and Russian content
    - Evidence quotes match source document lines
    - Works on both example input files
  </verify>
  <done>User types "approved" confirming the end-to-end pipeline produces correct, validated JSON output from markdown input.</done>
</task>

</tasks>

<verification>
1. `python -m dataset_generator generate --input <file> --out <dir> --seed 42` runs end-to-end
2. Output directory contains use_cases.json and policies.json
3. use_cases.json has UseCase objects with uc_-prefixed IDs and Russian content
4. policies.json has Policy objects with pol_-prefixed IDs, valid types, and Russian content
5. Evidence quotes match source document lines
6. Same seed produces structurally consistent output
7. Works on both example input files (anti-hardcoding)
</verification>

<success_criteria>
- CLI generate command runs end-to-end with real OpenAI API calls
- use_cases.json contains 5+ use cases with uc_ IDs, Russian names/descriptions, and evidence
- policies.json contains 5+ policies with pol_ IDs, valid types (must/must_not/escalate/style/format), Russian content, and evidence
- Evidence quotes are exact matches against source lines (validated by evidence_validator)
- Running twice with --seed 42 produces structurally consistent output (same counts, same ID patterns)
- Works on example_input_raw_support_faq_and_tickets.md AND example_input_raw_operator_quality_checks.md
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
