---
phase: 02-core-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/dataset_generator/extraction/use_case_extractor.py
  - src/dataset_generator/extraction/policy_extractor.py
autonomous: true

must_haves:
  truths:
    - "Use case extractor identifies use cases from prose, FAQs, tables, and implicit requirements without requiring explicit list formatting"
    - "Policy extractor classifies types correctly using decision tree (must, must_not, escalate, style, format)"
    - "Extraction prompts use generic patterns that do not reference specific document structures or filenames"
  artifacts:
    - path: "src/dataset_generator/extraction/use_case_extractor.py"
      provides: "Enhanced use case extraction with structured JSON prompt, few-shot examples, prose/table/implicit pattern recognition"
      contains: "IDENTIFICATION RULES"
    - path: "src/dataset_generator/extraction/policy_extractor.py"
      provides: "Enhanced policy extraction with decision tree classification, chain-of-thought, few-shot examples per type"
      contains: "DECISION TREE"
  key_links:
    - from: "src/dataset_generator/extraction/use_case_extractor.py"
      to: "src/dataset_generator/extraction/llm_client.py"
      via: "call_openai_structured with UseCaseList"
      pattern: "call_openai_structured"
    - from: "src/dataset_generator/extraction/policy_extractor.py"
      to: "src/dataset_generator/extraction/llm_client.py"
      via: "call_openai_structured with PolicyList"
      pattern: "call_openai_structured"
---

<objective>
Enhance use case and policy extraction prompts to handle unstructured text with improved evidence accuracy and reliable policy type classification.

Purpose: Phase 1 extraction works on structured markdown with explicit lists but needs improvement for prose-heavy, implicit requirements. Enhanced prompts with structured JSON format, few-shot examples, and a policy type decision tree will improve extraction quality and evidence accuracy from ~80% to >90%.

Output: Updated use_case_extractor.py and policy_extractor.py with production-quality prompts.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-extraction/02-RESEARCH.md
@.planning/phases/01-foundation/01-03-SUMMARY.md

@src/dataset_generator/extraction/use_case_extractor.py
@src/dataset_generator/extraction/policy_extractor.py
@src/dataset_generator/extraction/llm_client.py
@src/dataset_generator/extraction/markdown_parser.py
@src/dataset_generator/models/use_case.py
@src/dataset_generator/models/policy.py
@src/dataset_generator/models/evidence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance use case extractor with structured prompt and few-shot examples</name>
  <files>src/dataset_generator/extraction/use_case_extractor.py</files>
  <action>
Replace the existing system_prompt in extract_use_cases() with an enhanced prompt that follows the patterns from 02-RESEARCH.md Pattern 1. The new prompt must:

1. Use structured JSON format for task definition (objective, constraints, evidence rules) for clarity
2. Include USE CASE IDENTIFICATION RULES that describe generic patterns for finding use cases:
   - Action/scenario patterns: "может...", "должен...", "если...то..."
   - Question-answer pairs (FAQ sections)
   - Table rows with operator responses
   - Implicit use cases in prose (extract intent from context)
   - Each distinct user goal or system behavior = one use case
3. Include EVIDENCE EXTRACTION section (CRITICAL) with explicit formatting preservation:
   - CHARACTER-FOR-CHARACTER EXACT quotes required
   - Include ALL markdown formatting: *, **, bullets, table pipes |
   - Preserve ALL whitespace at start/end of lines
   - Do NOT clean, normalize, or "fix" the quote
   - Multi-line quotes: use \n between lines, preserve each line exactly
   - Line numbers shown as "N: " prefix — use for line_start/line_end but do NOT include in quote
   - Extract the COMPLETE quote — do not truncate or summarize
4. Include 2-3 FEW-SHOT EXAMPLES covering:
   - Explicit use case from a bullet list (1 line)
   - Implicit use case from prose spanning 2 lines
   - Use case from a table row (preserving pipe characters)
5. Use generic terms ("document", "text", "requirements") — do NOT reference specific sections like "FAQ" or "tickets" in the prompt template itself
6. Keep the min_use_cases parameter in the task definition JSON

Do NOT change the function signature, return type, evidence validation call, or logging. Only replace the system_prompt string.
  </action>
  <verify>
Run: `python -c "from dataset_generator.extraction.use_case_extractor import extract_use_cases; print('Import OK')"` — must succeed.

Verify prompt contains: "IDENTIFICATION RULES", "CHARACTER-FOR-CHARACTER", at least 2 example JSON blocks, and no hardcoded document-specific references like "FAQ", "ticket_id", or specific filenames in the prompt template.
  </verify>
  <done>
use_case_extractor.py has enhanced prompt with structured JSON task definition, generic identification rules for prose/table/implicit patterns, explicit evidence formatting preservation instructions, and 2-3 few-shot examples. Function signature and behavior unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance policy extractor with decision tree and chain-of-thought classification</name>
  <files>src/dataset_generator/extraction/policy_extractor.py</files>
  <action>
Replace the existing system_prompt in extract_policies() with an enhanced prompt that follows 02-RESEARCH.md Pattern 3. The new prompt must:

1. Include POLICY TYPE DECISION TREE with exclusive criteria (ordered for disambiguation):
   - Step 1: Is this a prohibition/restriction? → type: "must_not"
   - Step 2: Does this trigger human escalation? → type: "escalate"
   - Step 3: Is this about communication tone/style? → type: "style"
   - Step 4: Is this about output format/structure/templates? → type: "format"
   - Step 5: Otherwise (obligation/requirement) → type: "must"
   The order matters: check must_not and escalate BEFORE must, since they're special cases of "must" that need priority.

2. Include CLASSIFICATION PROCESS instruction asking LLM to reason through the decision tree before choosing type (chain-of-thought):
   - "For each policy, explain your reasoning in 1 sentence before assigning the type"

3. Include 2-3 FEW-SHOT EXAMPLES per type (at least 5 total, one per type) showing:
   - The policy statement (in Russian)
   - The reasoning/analysis
   - The assigned type
   Examples should use Russian text matching typical patterns from business requirement documents.

4. Include the same EVIDENCE EXTRACTION section as use_case_extractor (same formatting preservation rules, same emphasis on CHARACTER-EXACT quotes, same multi-line handling)

5. Use generic terms — do NOT reference specific document types or structures in the prompt template

6. Keep the min_policies parameter in the structured task definition

Do NOT change the function signature, return type, evidence validation call, or logging. Only replace the system_prompt string.
  </action>
  <verify>
Run: `python -c "from dataset_generator.extraction.policy_extractor import extract_policies; print('Import OK')"` — must succeed.

Verify prompt contains: "DECISION TREE", "must_not" before "must" in the decision tree order, at least 5 classification examples (one per type), "CLASSIFICATION PROCESS" or "REASONING" instruction, and no hardcoded document-specific references.
  </verify>
  <done>
policy_extractor.py has enhanced prompt with ordered decision tree (must_not and escalate checked before must), chain-of-thought classification instruction, 5+ few-shot examples (one per type) with Russian text, and explicit evidence formatting preservation. Function signature and behavior unchanged.
  </done>
</task>

</tasks>

<verification>
1. Both extractors import successfully: `python -c "from dataset_generator.extraction.use_case_extractor import extract_use_cases; from dataset_generator.extraction.policy_extractor import extract_policies; print('Both OK')"`
2. No hardcoded document references in prompts: grep for "FAQ", "ticket", "support" in the system_prompt strings — should not appear in prompt templates
3. Policy decision tree has correct priority order: must_not and escalate checked before generic must
4. Both prompts include evidence formatting preservation instructions
5. Both prompts include few-shot examples with Russian text
</verification>

<success_criteria>
- Use case extractor prompt handles prose, tables, and implicit requirements via generic identification rules
- Policy extractor correctly prioritizes type classification via ordered decision tree
- Both prompts include explicit evidence formatting preservation (CHARACTER-EXACT)
- Both prompts include few-shot examples with Russian text
- No document-specific hardcoding in prompt templates
- All existing tests and imports still work
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-extraction/02-01-SUMMARY.md`
</output>
